{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_handling\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRECTORY = \"./CelebDataProcessed\"\n",
    "ANNOTATIONS_DIRECTORY = \"./annotations.csv\"\n",
    "NAME = \"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "TRANSFORM = torchvision.transforms.Compose([\n",
    "torchvision.transforms.ToPILImage(),\n",
    "torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Get the dataset\n",
    "pubfig = data_handling.PublicFigureDataset(ANNOTATIONS_DIRECTORY, DATASET_DIRECTORY, NAME, transform=TRANSFORM)\n",
    "\n",
    "# 80-20 train test split\n",
    "train_size = int(0.8 * len(pubfig))\n",
    "test_size = len(pubfig) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(pubfig, [train_size, test_size])\n",
    "\n",
    "# Create the dataloaders\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a = data_handling.PublicFigureDataset(ANNOTATIONS_DIRECTORY, DATASET_DIRECTORY, \"\", transform=TRANSFORM)\n",
    "dataset_b = data_handling.PublicFigureDataset(ANNOTATIONS_DIRECTORY, DATASET_DIRECTORY, \"\", transform=TRANSFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_a = int(0.8 * len(dataset_a))\n",
    "test_size_a = len(dataset_a) - train_size_a\n",
    "train_dataset_a, test_dataset_a = torch.utils.data.random_split(dataset_a, [train_size_a, test_size_a])\n",
    "\n",
    "train_size_b = int(0.8 * len(dataset_b))\n",
    "test_size_b = len(dataset_b) - train_size_b\n",
    "train_dataset_b, test_dataset_b = torch.utils.data.random_split(dataset_b, [train_size_b, test_size_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_a = DataLoader(train_dataset_a, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl_a = DataLoader(test_dataset_a, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_dl_b = DataLoader(train_dataset_b, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl_b = DataLoader(test_dataset_b, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Focused\"\n",
    "LATENT_DIM = 100\n",
    "EPOCHS = 10_000\n",
    "DISCRIM = True\n",
    "\n",
    "VIDEO_PATH = \"./Outputs/Video/\" + MODEL_NAME # The directory should have a folder named \"/a/\" and another named \"/b/\" for each decoder\n",
    "if not os.path.exists(VIDEO_PATH):\n",
    "    os.makedirs(VIDEO_PATH)\n",
    "newpath = VIDEO_PATH + \"/a/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = VIDEO_PATH + \"/b/\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SingleEnc(LATENT_DIM, discriminator=DISCRIM).to(device)\n",
    "# model = models.ModifiedSingleEnc(LATENT_DIM, discriminator=DISCRIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "lr= 0.001\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-05)\n",
    "optimDis = torch.optim.Adam(model.parameters(), lr=.01, weight_decay=1e-05)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optim, [5, 10], 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "import pandas as pd\n",
    "\n",
    "# Initializes the .csv file\n",
    "def initializeCSV(model_name, col_names=['Epoch', 'Avg_Train_lossA', 'Avg_Train_lossB', 'TestLossA', 'TestLossB']):\n",
    "    csv = pd.DataFrame(columns=col_names)\n",
    "    csv.to_csv('./Outputs/CSV/' + model_name + '.csv', index=False)\n",
    "\n",
    "def writeToCSV(model_name, data):\n",
    "    with open('./Outputs/CSV/' + model_name + '.csv', 'a') as file:\n",
    "        writer_object = writer(file, lineterminator = '\\n')\n",
    "        writer_object.writerow(data)\n",
    "        file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptual Loss (Unused)\n",
    "\n",
    "The use of an additional classifier model for perceptual loss causes OutOfMemoryErrors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "LossOutput = namedtuple(\"LossOutput\", [\"conv2d_2b\", \"conv2d_4b\", \"last_linear\", \"logits\"])\n",
    "\n",
    "# https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n",
    "class LossNetwork(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(LossNetwork, self).__init__()\n",
    "        self.model = model\n",
    "        self.layer_name_mapping = {\n",
    "            '3': \"conv2d_2b\",\n",
    "            '8': \"conv2d_4b\",\n",
    "            #'15': \"last_linear\",\n",
    "            #'22': \"logits\"\n",
    "        }\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layer_name_mapping:\n",
    "                output[self.layer_name_mapping[name]] = x\n",
    "        return LossOutput(**output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "# Model pretrained on VGGFace2\n",
    "r = InceptionResnetV1(pretrained='vggface2').to(device).eval()\n",
    "loss_network = LossNetwork(r)\n",
    "loss_network.eval()\n",
    "del r\n",
    "# Takes images of size 160px by 160px as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loss_network\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').to(device).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "from numpy.random import randint\n",
    "import time\n",
    "import gc\n",
    "import helper as h\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, model_name, device, trainloader_a, trainloader_b, loss_fn, optimizer, testloader_a, testloader_b, dataset_a, dataset_b, epochs=5, default_dtype=torch.FloatTensor, video_path=\"\", scheduler=None, discriminator=False, discriminatorOpt=None):\n",
    "\n",
    "    if discriminator:\n",
    "        initializeCSV(model_name, col_names=['Epoch', 'Avg_Train_lossA', 'Avg_Train_lossB', 'TestLossA', 'TestLossB', 'DiscrimA', 'DiscrimB', 'GenA', 'GenB'])\n",
    "    else:\n",
    "        initializeCSV(model_name)\n",
    "    start_time = time.time()\n",
    "    iters = 0\n",
    "\n",
    "    if len(video_path) > 0:\n",
    "        # Get an image from dataset A\n",
    "        index = randint(len(dataset_a)) # From the dataset we get a random image, TODO: Feed it a specific control image\n",
    "        image_a, name = h.getImage(index, dataset_a) \n",
    "        image_a = image_a.unsqueeze(0).to(device)\n",
    "        save_image(image_a, video_path + \"/{}a.png\".format(name))\n",
    "\n",
    "        # Get an image from dataset B\n",
    "        index = randint(len(dataset_b))\n",
    "        image_b, name = h.getImage(index, dataset_b) \n",
    "        image_b = image_b.unsqueeze(0).to(device)\n",
    "        save_image(image_b, video_path + \"/{}b.png\".format(name))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        train_loss_a = []\n",
    "        train_loss_b = []\n",
    "        if discriminator:\n",
    "            gen_loss_a = []\n",
    "            gen_loss_b = []\n",
    "            dis_loss_a = []\n",
    "            dis_loss_b = []\n",
    "\n",
    "\n",
    "\n",
    "        # Use the first trainloader to train decoder A\n",
    "        for i, (image_batch, _) in enumerate(trainloader_a): # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "\n",
    "            # Records the images\n",
    "            if len(video_path) > 0:\n",
    "                model.eval()\n",
    "                output = model.decode(model.encode(image_a), \"a\")\n",
    "                save_image(output, video_path + \"/a/{}_{}.png\".format(ep, i))\n",
    "                model.train()\n",
    "\n",
    "            iters += 1\n",
    "\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.type(default_dtype).to(device)\n",
    "\n",
    "            #######################\n",
    "            if discriminator:\n",
    "                loss_a, discriminator_loss_a, generator_loss_a = trainStep(model, image_batch, loss_fn, optimizer, \"a\", discriminator, discriminatorOpt)\n",
    "                #loss_b, discriminator_loss_b, generator_loss_b = trainStep(model, image_batch, loss_fn, optimizer, \"b\", discriminator, discriminatorOpt)\n",
    "\n",
    "                gen_loss_a.append(generator_loss_a.detach().cpu().numpy())\n",
    "                #gen_loss_b.append(generator_loss_b.detach().cpu().numpy())\n",
    "\n",
    "                dis_loss_a.append(discriminator_loss_a.detach().cpu().numpy())\n",
    "                #dis_loss_b.append(discriminator_loss_b.detach().cpu().numpy())\n",
    "\n",
    "            else:\n",
    "                loss_a = trainStep(model, image_batch, loss_fn, optimizer, \"a\", discriminator, discriminatorOpt)\n",
    "                #loss_b = trainStep(model, image_batch, loss_fn, optimizer, \"b\", discriminator, discriminatorOpt)\n",
    "            ########################\n",
    "\n",
    "            time_lapse = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n",
    "            if i % 20 == 0:\n",
    "                print('Epoch:{:2d}A | Iter:{:5d} | Time: {} | Train_A Loss: {:.4f}'.format(ep+1, i, time_lapse, loss_a.data))\n",
    "\n",
    "            # Print batch loss\n",
    "            train_loss_a.append(loss_a.detach().cpu().numpy())\n",
    "            #train_loss_b.append(loss_b.detach().cpu().numpy())\n",
    "\n",
    "        # Use trainloader B to train decoder B\n",
    "        for i, (image_batch, _) in enumerate(trainloader_b):\n",
    "\n",
    "            # Records the images\n",
    "            if len(video_path) > 0:\n",
    "                model.eval()\n",
    "                output = model.decode(model.encode(image_b), \"b\")\n",
    "                save_image(output, video_path + \"/b/{}_{}.png\".format(ep, i))\n",
    "                model.train()\n",
    "\n",
    "            iters += 1\n",
    "\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.type(default_dtype).to(device)\n",
    "\n",
    "            #######################\n",
    "            if discriminator:\n",
    "                loss_b, discriminator_loss_b, generator_loss_b = trainStep(model, image_batch, loss_fn, optimizer, \"b\", discriminator, discriminatorOpt)\n",
    "\n",
    "                gen_loss_b.append(generator_loss_b.detach().cpu().numpy())\n",
    "\n",
    "                dis_loss_b.append(discriminator_loss_b.detach().cpu().numpy())\n",
    "\n",
    "            else:\n",
    "                #loss_a = trainStep(model, image_batch, loss_fn, optimizer, \"a\", discriminator, discriminatorOpt)\n",
    "                loss_b = trainStep(model, image_batch, loss_fn, optimizer, \"b\", discriminator, discriminatorOpt)\n",
    "            ########################\n",
    "\n",
    "            time_lapse = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n",
    "            if i % 20 == 0:\n",
    "                print('Epoch:{:2d}B | Iter:{:5d} | Time: {} | Train_B Loss: {:.4f}'.format(ep+1, i, time_lapse, loss_b.data))\n",
    "\n",
    "            # Print batch loss\n",
    "            train_loss_b.append(loss_b.detach().cpu().numpy())\n",
    "            #train_loss_b.append(loss_b.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        \n",
    "        gc.collect()\n",
    "        test_loss_a = test_epoch(model, device, testloader_a, loss_fn, type=\"a\")\n",
    "        test_loss_b = test_epoch(model, device, testloader_b, loss_fn, type=\"b\")\n",
    "        print('\\n EPOCH {}/{} \\t Avg. Train_A loss this Epoch {} \\t Avg. Train_B loss this Epoch {} \\t Test loss A {} \\t Test loss B {}'.format(ep + 1, epochs, np.mean(train_loss_a),np.mean(train_loss_b), test_loss_a, test_loss_b))\n",
    "\n",
    "        scheduler.step()\n",
    "        # Write the losses to a .csv file\n",
    "        if discriminator:\n",
    "            data = [ep + 1, np.mean(train_loss_a), np.mean(train_loss_b), test_loss_a.item(), test_loss_b.item(), np.mean(dis_loss_a), np.mean(dis_loss_b), np.mean(gen_loss_a), np.mean(gen_loss_b)]\n",
    "            writeToCSV(model_name, data)\n",
    "        else:\n",
    "            data = [ep + 1, np.mean(train_loss_a), np.mean(train_loss_b), test_loss_a.item(), test_loss_b.item()]\n",
    "            writeToCSV(model_name, data)\n",
    "\n",
    "    return\n",
    "\n",
    "def test_epoch(model, device, testloader, loss_fn, default_dtype=torch.FloatTensor, type=\"a\"):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        val_loss = testStep(model, testloader, loss_fn, device, default_dtype, type=type)\n",
    "        \n",
    "    return val_loss.data\n",
    "\n",
    "def testStep(model, testloader, lossFunction, device, default_dtype, type, discriminator=False):\n",
    "\n",
    "    conc_out = []\n",
    "    conc_label = []\n",
    "    for image_batch, _ in testloader:\n",
    "        # Move tensor to the proper device\n",
    "        image_batch = image_batch.type(default_dtype).to(device)\n",
    "        reconstruction = model.decode(model.encode(image_batch), type)\n",
    "\n",
    "        # Append the network output and the original image to the lists\n",
    "        conc_out.append(reconstruction.cpu())\n",
    "        conc_label.append(image_batch.cpu())\n",
    "\n",
    "    # Create a single tensor with all the values in the lists\n",
    "    conc_out = torch.cat(conc_out)\n",
    "    conc_label = torch.cat(conc_label) \n",
    "\n",
    "    # Evaluate global loss\n",
    "    val_loss = lossFunction(conc_out, conc_label)\n",
    "    if discriminator:\n",
    "        with torch.no_grad():\n",
    "            fake_labels = model.discriminator(reconstruction, type)\n",
    "        generator_loss = torch.mean(-torch.log(fake_labels))\n",
    "        val_loss = val_loss + generator_loss\n",
    "    return val_loss\n",
    "\n",
    "def trainStep(model, batch, loss_fn, optimizer, type, discrim, discriminatorOptimizer):\n",
    "    reconstruction = model.decode(model.encode(batch), type)\n",
    "    reconstruction_loss = loss_fn(reconstruction, batch)\n",
    "\n",
    "    if(discrim):\n",
    "        with torch.no_grad():\n",
    "            fake_labels = model.discriminator(reconstruction, type)\n",
    "        generator_loss = torch.mean(-torch.log(fake_labels + 1e-12))\n",
    "\n",
    "        # Evaluate loss\n",
    "        loss = reconstruction_loss + generator_loss\n",
    "    else:\n",
    "        loss = reconstruction_loss\n",
    "\n",
    "    ###############Perceptual Loss###############\n",
    "    #transform = torchvision.transforms.Resize(160)\n",
    "    #resizedBatch = transform(batch)\n",
    "    #resizedReconstruction = transform(reconstruction)\n",
    "\n",
    "    #features_y = resnet(resizedBatch)\n",
    "    #features_xc = resnet(resizedReconstruction)\n",
    "\n",
    "    #f_xc_c = Variable(features_xc[1].data, requires_grad=False)\n",
    "\n",
    "    #content_loss = loss_fn(features_y, features_xc)\n",
    "    #loss = loss + content_loss\n",
    "    ##############################################\n",
    "\n",
    "    # Backward pass for autoencoder\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    if(discrim):\n",
    "        fake_labels = model.discriminator(reconstruction.detach(), type)\n",
    "        real_labels = model.discriminator(batch.detach(), type)\n",
    "        \n",
    "        discriminator_loss = torch.mean(-(torch.log(real_labels + 1e-12) + torch.log(1 - fake_labels + 1e-12)))\n",
    "\n",
    "        discriminatorOptimizer.zero_grad()\n",
    "        discriminator_loss.backward(retain_graph=True)\n",
    "        discriminatorOptimizer.step()\n",
    "        return reconstruction_loss, discriminator_loss, generator_loss\n",
    "        \n",
    "    return reconstruction_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kam Nanthanolath\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KAMNAN~1\\AppData\\Local\\Temp/ipykernel_17180/3109938082.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dl_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dl_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVIDEO_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mDISCRIM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminatorOpt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimDis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\KAMNAN~1\\AppData\\Local\\Temp/ipykernel_17180/971133236.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, model_name, device, trainloader_a, trainloader_b, loss_fn, optimizer, testloader_a, testloader_b, dataset_a, dataset_b, epochs, default_dtype, video_path, scheduler, discriminator, discriminatorOpt)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;31m#######################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[0mloss_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator_loss_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator_loss_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminatorOpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[1;31m#loss_b, discriminator_loss_b, generator_loss_b = trainStep(model, image_batch, loss_fn, optimizer, \"b\", discriminator, discriminatorOpt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\KAMNAN~1\\AppData\\Local\\Temp/ipykernel_17180/971133236.py\u001b[0m in \u001b[0;36mtrainStep\u001b[1;34m(model, batch, loss_fn, optimizer, type, discrim, discriminatorOptimizer)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mdiscriminatorOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[0mdiscriminator_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[0mdiscriminatorOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreconstruction_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_epoch(model, MODEL_NAME, device, train_dl_a, train_dl_a, loss_fn, optim, test_dl_a, test_dl_b, dataset_a, dataset_b, epochs=EPOCHS, video_path=VIDEO_PATH, scheduler=scheduler, discriminator= DISCRIM, discriminatorOpt=optimDis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h.saveWeights(model, \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SingleEnc(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(9, 9), stride=(4, 4))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout2d(p=0.5, inplace=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.01)\n",
      "    (7): Dropout2d(p=0.5, inplace=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.01)\n",
      "    (11): Dropout2d(p=0.5, inplace=False)\n",
      "    (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.01)\n",
      "    (15): Dropout2d(p=0.5, inplace=False)\n",
      "    (16): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (17): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): LeakyReLU(negative_slope=0.01)\n",
      "    (19): Dropout2d(p=0.5, inplace=False)\n",
      "    (20): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (21): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.01)\n",
      "    (23): Dropout2d(p=0.5, inplace=False)\n",
      "    (24): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (inter): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=4096, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=4096, out_features=100, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=100, out_features=4096, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=4096, out_features=8192, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (decoderA): Sequential(\n",
      "    (0): Unflatten(dim=1, unflattened_size=(512, 4, 4))\n",
      "    (1): ConvTranspose2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Dropout2d(p=0.5, inplace=False)\n",
      "    (5): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Dropout2d(p=0.5, inplace=False)\n",
      "    (9): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Dropout2d(p=0.5, inplace=False)\n",
      "    (13): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Dropout2d(p=0.5, inplace=False)\n",
      "    (17): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Dropout2d(p=0.5, inplace=False)\n",
      "    (21): ConvTranspose2d(64, 3, kernel_size=(9, 9), stride=(4, 4))\n",
      "    (22): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): Sigmoid()\n",
      "    (24): Dropout2d(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoderB): Sequential(\n",
      "    (0): Unflatten(dim=1, unflattened_size=(512, 4, 4))\n",
      "    (1): ConvTranspose2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Dropout2d(p=0.5, inplace=False)\n",
      "    (5): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Dropout2d(p=0.5, inplace=False)\n",
      "    (9): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Dropout2d(p=0.5, inplace=False)\n",
      "    (13): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Dropout2d(p=0.5, inplace=False)\n",
      "    (17): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Dropout2d(p=0.5, inplace=False)\n",
      "    (21): ConvTranspose2d(64, 3, kernel_size=(9, 9), stride=(4, 4))\n",
      "    (22): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): Sigmoid()\n",
      "    (24): Dropout2d(p=0.5, inplace=False)\n",
      "  )\n",
      "  (discriminatorA): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      "  (discriminatorB): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
