{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Note that this notebook isn't meant to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_handling\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Dataset Produced from: https://www.cs.columbia.edu/CAVE/databases/pubfig/\n",
    "\n",
    "Downloaded from: https://www.kaggle.com/datasets/kaustubhchaudhari/pubfig-dataset-256x256-jpg\n",
    "\n",
    "The public figure dataset allegedly has 58,797 images of public figures, however the actual downloaded dataset is 11,640 images. The images are colored and 256px by 256px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRECTORY = \"./CelebDataProcessed\"\n",
    "ANNOTATIONS_DIRECTORY = \"./annotations.csv\"\n",
    "NAME = \"\"\n",
    "BATCH_SIZE = 64\n",
    "TRANSFORM = torchvision.transforms.Compose([\n",
    "torchvision.transforms.ToPILImage(),\n",
    "torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pubfig = data_handling.PublicFigureDataset(ANNOTATIONS_DIRECTORY, DATASET_DIRECTORY, NAME, transform=TRANSFORM)\n",
    "\n",
    "# 80-20 train test split\n",
    "train_size = int(0.8 * len(pubfig))\n",
    "test_size = len(pubfig) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(pubfig, [train_size, test_size])\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "The basis for deepfakes is to use autoencoders to extract the core features person A into a latent space, and then decode it with a decoder trained on person B. This first model was what I used to test the success and capabilities of autoencoders trained on the public figure dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import encdec as ed\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, layer_count, latent_dim, input_dim, dropout_odds=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = ed.Encoder(layer_count, latent_dim, input_dim, dropout_odds=dropout_odds)\n",
    "        self.decoder = ed.Decoder(layer_count, latent_dim, input_dim, self.encoder.getOut(), dropout_odds=dropout_odds)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        y = self.decode(z)\n",
    "        return y\n",
    "\n",
    "    def encode(self, x):\n",
    "        output = self.encoder(x)\n",
    "        return output\n",
    "\n",
    "    def decode(self, input):\n",
    "        y = self.decoder(input)\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "This model is based on the model from the Deepfacelab paper (https://paperswithcode.com/paper/deepfacelab-a-simple-flexible-and-extensible). Source Code: https://github.com/iperov/DeepFaceLab\n",
    "\n",
    "And the model from Faceswap-GAN (https://github.com/shaoanlu/faceswap-GAN/blob/master/networks/faceswap_gan_model.py).\n",
    "\n",
    "The general structure of the models is to utilize a single encoder and two decoders, with each decoder trained on a different person. This allows the two faces to be correctly encoded into the same latent space, and as such allow the decoders to decode a different person's face onto the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleEnc(nn.Module):\n",
    "    def __init__(self, latent_dim, leak=True, dropout_odds=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 9, 4),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.Conv2d(64, 128, 5, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.Conv2d(128, 256, 5, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.Conv2d(512, 1024, 3, 2, 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.Conv2d(1024, 512, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.inter = nn.Sequential(\n",
    "            nn.Linear(8192, 2048),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Linear(2048, latent_dim),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Linear(latent_dim, 2048),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Linear(2048, 8192),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoderA = nn.Sequential(\n",
    "            nn.Unflatten(dim=1, unflattened_size=(512, 4, 4)),\n",
    "            nn.ConvTranspose2d(512, 1024, 1, 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.ConvTranspose2d(1024, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 5, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 5, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, 9, 4),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "        )\n",
    "        self.decoderB = nn.Sequential(\n",
    "            nn.Unflatten(dim=1, unflattened_size=(512, 4, 4)),\n",
    "            nn.ConvTranspose2d(512, 1024, 1, 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.ConvTranspose2d(1024, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 5, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 5, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU() if leak else nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, 9, 4),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout2d(p=dropout_odds),\n",
    "        )\n",
    "        #self.discriminatorA = nn.Sequential(\n",
    "\n",
    "        #)\n",
    "        #self.discriminatorB = nn.Sequential(\n",
    "            \n",
    "        #)\n",
    "\n",
    "    def forward(self, x, type='a'):\n",
    "        x = self.encoder(x)\n",
    "        x = self.inter(x)\n",
    "        if type == 'a':\n",
    "            x = self.decoderA(x)\n",
    "        else:\n",
    "            x = self.decoderB(x)\n",
    "        x = torchvision.transforms.Resize((256, 256))(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike model 1 which has a rather standard training procedure, model 2 requires two backward passes because of the decoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "from numpy.random import randint\n",
    "import time\n",
    "import gc\n",
    "\n",
    "def train_epoch(model, device, trainloader, loss_fn, optimizer, testloader, dataset, epochs=5, default_dtype=torch.FloatTensor, video=False):\n",
    "\n",
    "    start_time = time.time()\n",
    "    iters = 0\n",
    "\n",
    "    if video:\n",
    "        index = randint(len(dataset)) # From the dataset we get a random image\n",
    "        image, name = h.getImage(index, dataset) \n",
    "        image = image.unsqueeze(0)\n",
    "        save_image(image, \"./Outputs/Video/deepfaker/{}.png\".format(name))\n",
    "\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        train_loss_a = []\n",
    "        train_loss_b = []\n",
    "\n",
    "        # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "        for i, (image_batch, _) in enumerate(trainloader): # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "            if video: # TODO: This only works for the Autoencoder class atm\n",
    "                model.eval()\n",
    "                output = model(image, \"a\")\n",
    "                save_image(output, \"./Outputs/Video/deepfaker/a/{}_{}.png\".format(ep, i))\n",
    "                output = model(image, \"b\")\n",
    "                save_image(output, \"./Outputs/Video/deepfaker/b/{}_{}.png\".format(ep, i))\n",
    "                model.train()\n",
    "\n",
    "            iters += 1\n",
    "\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.type(default_dtype).to(device)\n",
    "            #labels = labels.type(default_dtype).to(device)\n",
    "\n",
    "            # Encode data\n",
    "            output = model(image_batch, type=\"a\")\n",
    "\n",
    "            # Evaluate loss\n",
    "            loss_a = loss_fn(output, image_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss_a.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            output = model(image_batch, type=\"b\")\n",
    "\n",
    "            # Evaluate loss\n",
    "            loss_b = loss_fn(output, image_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss_b.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            time_lapse = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n",
    "            if i % 20 == 0:\n",
    "                print('Epoch:{:2d} | Iter:{:5d} | Time: {} | Train_A Loss: {:.4f} | Train_B Loss: {:.4f}'.format(ep+1, i, time_lapse, loss_a.data, loss_b.data))\n",
    "\n",
    "            # Print batch loss\n",
    "            train_loss_a.append(loss_a.detach().cpu().numpy())\n",
    "            train_loss_b.append(loss_b.detach().cpu().numpy())\n",
    "        gc.collect()\n",
    "        test_loss_a, test_loss_b = test_epoch(model, device, testloader, loss_fn)\n",
    "        print('\\n EPOCH {}/{} \\t Avg. Train_A loss this Epoch {} \\t Avg. Train_B loss this Epoch {} \\t Test loss A {} \\t Test loss B {}'.format(ep + 1, epochs, np.mean(train_loss_a),np.mean(train_loss_b), test_loss_a, test_loss_b))\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def test_epoch(model, device, dataloader, loss_fn, default_dtype=torch.FloatTensor):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out_a = []\n",
    "        conc_label_a = []\n",
    "        for image_batch, _ in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.type(default_dtype).to(device)#image_batch.type(torch.HalfTensor).to(device)\n",
    "            # Encode data\n",
    "            output = model(image_batch)\n",
    "\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out_a.append(output.cpu())\n",
    "            conc_label_a.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out_a = torch.cat(conc_out_a)\n",
    "        conc_label_a = torch.cat(conc_label_a) \n",
    "        # Evaluate global loss\n",
    "        val_loss_a = loss_fn(conc_out_a, conc_label_a)\n",
    "\n",
    "        conc_out_b = []\n",
    "        conc_label_b = []\n",
    "        for image_batch, _ in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.type(default_dtype).to(device)#image_batch.type(torch.HalfTensor).to(device)\n",
    "            # Encode data\n",
    "            output = model(image_batch,  type=\"b\")\n",
    "\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out_b.append(output.cpu())\n",
    "            conc_label_b.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out_b = torch.cat(conc_out_b)\n",
    "        conc_label_b = torch.cat(conc_label_b) \n",
    "        # Evaluate global loss\n",
    "        val_loss_b = loss_fn(conc_out_b, conc_label_b)\n",
    "\n",
    "    return val_loss_a.data, val_loss_b.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "As of the creation of this notebook, the models are unable to successfully swap faces. In my attempt to train autoencoders, its become increasingly clear that whether it be due to time, hardware, or data constraints, the training process is extremely slow. During the training process, I ran an image through the model and saved the output images. I compiled them into the gifs below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an autoencoder trained on images of Donald Trump. Expectedly, due to the small amount of data it takes a significant amount of time before the model begins identifying facial features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](trump.gif \"segment\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a model trained on the entire pubfig dataset, demonstrating a much more defined face, however its still missing details, likely due to an insufficient training time. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](double.gif \"segment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a model pretrained on the entire dataset, then begins training only on a single individual. The gif begins with a clear image of a generalized face which the pretrained model learns, which it quickly discards and begins the learning process over. Unlike the Donald Trump model, this one quickly begins to form a face after a period of noise which is promising."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](png_to_gif.gif \"segment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although currently lacking the details to properly perform a face-swap, the results show the model is certainly capable of learning faces given sufficient time. The results also indicate that pretraining an autoencoder model on reconstructing faces in general can quicken the models subsequent learning of specific faces for one-to-one deepfake models. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
