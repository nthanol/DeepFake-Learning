{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_handling\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import encdec as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRECTORY = \"./CelebDataProcessed\"\n",
    "ANNOTATIONS_DIRECTORY = \"./annotations.csv\"\n",
    "NAME = \"\"\n",
    "BATCH_SIZE = 128\n",
    "TRANSFORM = torchvision.transforms.Compose([\n",
    "torchvision.transforms.ToPILImage(),\n",
    "torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pubfig = data_handling.PublicFigureDataset(ANNOTATIONS_DIRECTORY, DATASET_DIRECTORY, NAME, transform=TRANSFORM)\n",
    "\n",
    "# 80-20 train test split\n",
    "train_size = int(0.8 * len(pubfig))\n",
    "test_size = len(pubfig) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(pubfig, [train_size, test_size])\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Image Shape: torch.Size([3, 256, 256])\n",
      "Images are of: torch.float32\n",
      "Abhishek Bachan\n"
     ]
    }
   ],
   "source": [
    "### Check data\n",
    "index = 0 # First image from the dataset\n",
    "print(type(pubfig[index][0]))\n",
    "print(\"Image Shape: \" + str(pubfig[index][0].size())) # Image\n",
    "print(\"Images are of: \" + str(pubfig[index][0].dtype))\n",
    "print((pubfig[index][1])) # Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "\n",
    "model = models.SingleEnc(LATENT_DIM).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kam Nanthanolath\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import helper as h\n",
    "from numpy.random import randint\n",
    "\n",
    "#h.loadWeights(model, \"./Weights/double.pth\")\n",
    "index = randint(len(pubfig))\n",
    "image , _= h.getImage(index, pubfig)\n",
    "\n",
    "model.eval()\n",
    "output = model(image.unsqueeze(0).to(device)) # Images need to be unsqueezed since we use batch norm. The model expects batch size as a part of the image.\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "lr= 0.001\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "from numpy.random import randint\n",
    "import time\n",
    "import gc\n",
    "\n",
    "def train_epoch(model, device, trainloader, loss_fn, optimizer, testloader, dataset, epochs=5, default_dtype=torch.FloatTensor, video=False):\n",
    "\n",
    "    start_time = time.time()\n",
    "    iters = 0\n",
    "\n",
    "    if video:\n",
    "        index = randint(len(dataset)) # From the dataset we get a random image\n",
    "        image, name = h.getImage(index, dataset) \n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        save_image(image, \"./Outputs/Video/GPU/{}.png\".format(name))\n",
    "\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        train_loss_a = []\n",
    "        train_loss_b = []\n",
    "\n",
    "        # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "        for i, (image_batch, _) in enumerate(trainloader): # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "            if video: # TODO: This only works for the Autoencoder class atm\n",
    "                model.eval()\n",
    "                output = model(image, \"a\")\n",
    "                save_image(output, \"./Outputs/Video/GPU/a/{}_{}.png\".format(ep, i))\n",
    "                output = model(image, \"b\")\n",
    "                save_image(output, \"./Outputs/Video/GPU/b/{}_{}.png\".format(ep, i))\n",
    "                model.train()\n",
    "\n",
    "            iters += 1\n",
    "\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.type(default_dtype).to(device)\n",
    "            #labels = labels.type(default_dtype).to(device)\n",
    "\n",
    "            # Encode data\n",
    "            output = model(image_batch, type=\"a\")\n",
    "\n",
    "            # Evaluate loss\n",
    "            loss_a = loss_fn(output, image_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss_a.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            output = model(image_batch, type=\"b\")\n",
    "\n",
    "            # Evaluate loss\n",
    "            loss_b = loss_fn(output, image_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss_b.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            time_lapse = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n",
    "            if i % 20 == 0:\n",
    "                print('Epoch:{:2d} | Iter:{:5d} | Time: {} | Train_A Loss: {:.4f} | Train_B Loss: {:.4f}'.format(ep+1, i, time_lapse, loss_a.data, loss_b.data))\n",
    "\n",
    "            # Print batch loss\n",
    "            train_loss_a.append(loss_a.detach().cpu().numpy())\n",
    "            train_loss_b.append(loss_b.detach().cpu().numpy())\n",
    "        gc.collect()\n",
    "        test_loss_a, test_loss_b = test_epoch(model, device, testloader, loss_fn)\n",
    "        print('\\n EPOCH {}/{} \\t Avg. Train_A loss this Epoch {} \\t Avg. Train_B loss this Epoch {} \\t Test loss A {} \\t Test loss B {}'.format(ep + 1, epochs, np.mean(train_loss_a),np.mean(train_loss_b), test_loss_a, test_loss_b))\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def test_epoch(model, device, dataloader, loss_fn, default_dtype=torch.FloatTensor):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out_a = []\n",
    "        conc_label_a = []\n",
    "        for image_batch, _ in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.type(default_dtype).to(device)#image_batch.type(torch.HalfTensor).to(device)\n",
    "            # Encode data\n",
    "            output = model(image_batch)\n",
    "\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out_a.append(output.cpu())\n",
    "            conc_label_a.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out_a = torch.cat(conc_out_a)\n",
    "        conc_label_a = torch.cat(conc_label_a) \n",
    "        # Evaluate global loss\n",
    "        val_loss_a = loss_fn(conc_out_a, conc_label_a)\n",
    "\n",
    "        conc_out_b = []\n",
    "        conc_label_b = []\n",
    "        for image_batch, _ in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.type(default_dtype).to(device)#image_batch.type(torch.HalfTensor).to(device)\n",
    "            # Encode data\n",
    "            output = model(image_batch,  type=\"b\")\n",
    "\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out_b.append(output.cpu())\n",
    "            conc_label_b.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out_b = torch.cat(conc_out_b)\n",
    "        conc_label_b = torch.cat(conc_label_b) \n",
    "        # Evaluate global loss\n",
    "        val_loss_b = loss_fn(conc_out_b, conc_label_b)\n",
    "\n",
    "    return val_loss_a.data, val_loss_b.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train_A Loss: 0.3525 | Train_B Loss: 0.3269\n",
      "Epoch: 1 | Iter:   20 | Time: 00:00:13 | Train_A Loss: 0.3164 | Train_B Loss: 0.2973\n",
      "Epoch: 1 | Iter:   40 | Time: 00:00:25 | Train_A Loss: 0.2804 | Train_B Loss: 0.3031\n",
      "Epoch: 1 | Iter:   60 | Time: 00:00:38 | Train_A Loss: 0.2870 | Train_B Loss: 0.2719\n",
      "\n",
      " EPOCH 1/100 \t Avg. Train_A loss this Epoch 0.2920683026313782 \t Avg. Train_B loss this Epoch 0.2909405827522278 \t Test loss A 0.061915166676044464 \t Test loss B 0.06251343339681625\n",
      "Epoch: 2 | Iter:    0 | Time: 00:01:03 | Train_A Loss: 0.2777 | Train_B Loss: 0.2818\n",
      "Epoch: 2 | Iter:   20 | Time: 00:01:15 | Train_A Loss: 0.2748 | Train_B Loss: 0.2775\n",
      "Epoch: 2 | Iter:   40 | Time: 00:01:28 | Train_A Loss: 0.2677 | Train_B Loss: 0.2644\n",
      "Epoch: 2 | Iter:   60 | Time: 00:01:40 | Train_A Loss: 0.2712 | Train_B Loss: 0.2696\n",
      "\n",
      " EPOCH 2/100 \t Avg. Train_A loss this Epoch 0.2704046666622162 \t Avg. Train_B loss this Epoch 0.27042216062545776 \t Test loss A 0.06364387273788452 \t Test loss B 0.0637412741780281\n",
      "Epoch: 3 | Iter:    0 | Time: 00:02:05 | Train_A Loss: 0.2603 | Train_B Loss: 0.2698\n",
      "Epoch: 3 | Iter:   20 | Time: 00:02:18 | Train_A Loss: 0.2623 | Train_B Loss: 0.2506\n",
      "Epoch: 3 | Iter:   40 | Time: 00:02:31 | Train_A Loss: 0.2465 | Train_B Loss: 0.2432\n",
      "Epoch: 3 | Iter:   60 | Time: 00:02:44 | Train_A Loss: 0.2518 | Train_B Loss: 0.2583\n",
      "\n",
      " EPOCH 3/100 \t Avg. Train_A loss this Epoch 0.25268644094467163 \t Avg. Train_B loss this Epoch 0.25410187244415283 \t Test loss A 0.05787146836519241 \t Test loss B 0.058995749801397324\n",
      "Epoch: 4 | Iter:    0 | Time: 00:03:08 | Train_A Loss: 0.2466 | Train_B Loss: 0.2481\n",
      "Epoch: 4 | Iter:   20 | Time: 00:03:21 | Train_A Loss: 0.2433 | Train_B Loss: 0.2359\n",
      "Epoch: 4 | Iter:   40 | Time: 00:03:34 | Train_A Loss: 0.2456 | Train_B Loss: 0.2456\n",
      "Epoch: 4 | Iter:   60 | Time: 00:03:47 | Train_A Loss: 0.2410 | Train_B Loss: 0.2447\n",
      "\n",
      " EPOCH 4/100 \t Avg. Train_A loss this Epoch 0.2408892959356308 \t Avg. Train_B loss this Epoch 0.24136219918727875 \t Test loss A 0.06387382745742798 \t Test loss B 0.06460823863744736\n",
      "Epoch: 5 | Iter:    0 | Time: 00:04:11 | Train_A Loss: 0.2299 | Train_B Loss: 0.2433\n",
      "Epoch: 5 | Iter:   20 | Time: 00:04:24 | Train_A Loss: 0.2237 | Train_B Loss: 0.2353\n",
      "Epoch: 5 | Iter:   40 | Time: 00:04:36 | Train_A Loss: 0.2339 | Train_B Loss: 0.2308\n",
      "Epoch: 5 | Iter:   60 | Time: 00:04:49 | Train_A Loss: 0.2428 | Train_B Loss: 0.2471\n",
      "\n",
      " EPOCH 5/100 \t Avg. Train_A loss this Epoch 0.22881118953227997 \t Avg. Train_B loss this Epoch 0.23082639276981354 \t Test loss A 0.06628384441137314 \t Test loss B 0.06505702435970306\n",
      "Epoch: 6 | Iter:    0 | Time: 00:05:13 | Train_A Loss: 0.2344 | Train_B Loss: 0.2281\n",
      "Epoch: 6 | Iter:   20 | Time: 00:05:26 | Train_A Loss: 0.2327 | Train_B Loss: 0.2234\n",
      "Epoch: 6 | Iter:   40 | Time: 00:05:38 | Train_A Loss: 0.2121 | Train_B Loss: 0.2296\n",
      "Epoch: 6 | Iter:   60 | Time: 00:05:51 | Train_A Loss: 0.2154 | Train_B Loss: 0.2146\n",
      "\n",
      " EPOCH 6/100 \t Avg. Train_A loss this Epoch 0.22107405960559845 \t Avg. Train_B loss this Epoch 0.22290028631687164 \t Test loss A 0.07041621953248978 \t Test loss B 0.07026109844446182\n",
      "Epoch: 7 | Iter:    0 | Time: 00:06:15 | Train_A Loss: 0.2296 | Train_B Loss: 0.2219\n",
      "Epoch: 7 | Iter:   20 | Time: 00:06:28 | Train_A Loss: 0.2174 | Train_B Loss: 0.2179\n",
      "Epoch: 7 | Iter:   40 | Time: 00:06:41 | Train_A Loss: 0.2220 | Train_B Loss: 0.2182\n",
      "Epoch: 7 | Iter:   60 | Time: 00:06:53 | Train_A Loss: 0.2183 | Train_B Loss: 0.2105\n",
      "\n",
      " EPOCH 7/100 \t Avg. Train_A loss this Epoch 0.21515090763568878 \t Avg. Train_B loss this Epoch 0.21543709933757782 \t Test loss A 0.06680801510810852 \t Test loss B 0.06685084849596024\n",
      "Epoch: 8 | Iter:    0 | Time: 00:07:17 | Train_A Loss: 0.1988 | Train_B Loss: 0.2019\n",
      "Epoch: 8 | Iter:   20 | Time: 00:07:30 | Train_A Loss: 0.2277 | Train_B Loss: 0.2115\n",
      "Epoch: 8 | Iter:   40 | Time: 00:07:43 | Train_A Loss: 0.2061 | Train_B Loss: 0.2203\n",
      "Epoch: 8 | Iter:   60 | Time: 00:07:56 | Train_A Loss: 0.1930 | Train_B Loss: 0.2146\n",
      "\n",
      " EPOCH 8/100 \t Avg. Train_A loss this Epoch 0.2066715806722641 \t Avg. Train_B loss this Epoch 0.20780494809150696 \t Test loss A 0.07051164656877518 \t Test loss B 0.0710650086402893\n",
      "Epoch: 9 | Iter:    0 | Time: 00:08:20 | Train_A Loss: 0.2031 | Train_B Loss: 0.1965\n",
      "Epoch: 9 | Iter:   20 | Time: 00:08:32 | Train_A Loss: 0.2041 | Train_B Loss: 0.2031\n",
      "Epoch: 9 | Iter:   40 | Time: 00:08:45 | Train_A Loss: 0.1961 | Train_B Loss: 0.2012\n",
      "Epoch: 9 | Iter:   60 | Time: 00:08:58 | Train_A Loss: 0.1899 | Train_B Loss: 0.2093\n",
      "\n",
      " EPOCH 9/100 \t Avg. Train_A loss this Epoch 0.20390336215496063 \t Avg. Train_B loss this Epoch 0.20209431648254395 \t Test loss A 0.07423488795757294 \t Test loss B 0.0755581259727478\n",
      "Epoch:10 | Iter:    0 | Time: 00:09:22 | Train_A Loss: 0.2193 | Train_B Loss: 0.2010\n",
      "Epoch:10 | Iter:   20 | Time: 00:09:35 | Train_A Loss: 0.2140 | Train_B Loss: 0.2149\n",
      "Epoch:10 | Iter:   40 | Time: 00:09:48 | Train_A Loss: 0.2008 | Train_B Loss: 0.2037\n",
      "Epoch:10 | Iter:   60 | Time: 00:10:01 | Train_A Loss: 0.1931 | Train_B Loss: 0.2263\n",
      "\n",
      " EPOCH 10/100 \t Avg. Train_A loss this Epoch 0.20017586648464203 \t Avg. Train_B loss this Epoch 0.20029695332050323 \t Test loss A 0.07631976902484894 \t Test loss B 0.07734745740890503\n",
      "Epoch:11 | Iter:    0 | Time: 00:10:25 | Train_A Loss: 0.1926 | Train_B Loss: 0.1861\n",
      "Epoch:11 | Iter:   20 | Time: 00:10:38 | Train_A Loss: 0.1890 | Train_B Loss: 0.1874\n",
      "Epoch:11 | Iter:   40 | Time: 00:10:51 | Train_A Loss: 0.2016 | Train_B Loss: 0.2005\n",
      "Epoch:11 | Iter:   60 | Time: 00:11:04 | Train_A Loss: 0.1874 | Train_B Loss: 0.2067\n",
      "\n",
      " EPOCH 11/100 \t Avg. Train_A loss this Epoch 0.19566068053245544 \t Avg. Train_B loss this Epoch 0.19658051431179047 \t Test loss A 0.07599815726280212 \t Test loss B 0.07897407561540604\n",
      "Epoch:12 | Iter:    0 | Time: 00:11:28 | Train_A Loss: 0.1881 | Train_B Loss: 0.2130\n",
      "Epoch:12 | Iter:   20 | Time: 00:11:41 | Train_A Loss: 0.1825 | Train_B Loss: 0.1976\n",
      "Epoch:12 | Iter:   40 | Time: 00:11:54 | Train_A Loss: 0.1860 | Train_B Loss: 0.1776\n",
      "Epoch:12 | Iter:   60 | Time: 00:12:07 | Train_A Loss: 0.1915 | Train_B Loss: 0.1931\n",
      "\n",
      " EPOCH 12/100 \t Avg. Train_A loss this Epoch 0.19415083527565002 \t Avg. Train_B loss this Epoch 0.19385109841823578 \t Test loss A 0.07891982048749924 \t Test loss B 0.07920050621032715\n",
      "Epoch:13 | Iter:    0 | Time: 00:12:32 | Train_A Loss: 0.1979 | Train_B Loss: 0.1844\n",
      "Epoch:13 | Iter:   20 | Time: 00:12:45 | Train_A Loss: 0.2008 | Train_B Loss: 0.1918\n",
      "Epoch:13 | Iter:   40 | Time: 00:12:58 | Train_A Loss: 0.1897 | Train_B Loss: 0.1954\n",
      "Epoch:13 | Iter:   60 | Time: 00:13:11 | Train_A Loss: 0.1837 | Train_B Loss: 0.2087\n",
      "\n",
      " EPOCH 13/100 \t Avg. Train_A loss this Epoch 0.18981106579303741 \t Avg. Train_B loss this Epoch 0.1915198564529419 \t Test loss A 0.08260121941566467 \t Test loss B 0.0861966535449028\n",
      "Epoch:14 | Iter:    0 | Time: 00:13:35 | Train_A Loss: 0.1872 | Train_B Loss: 0.1982\n",
      "Epoch:14 | Iter:   20 | Time: 00:13:48 | Train_A Loss: 0.1945 | Train_B Loss: 0.1852\n",
      "Epoch:14 | Iter:   40 | Time: 00:14:01 | Train_A Loss: 0.1890 | Train_B Loss: 0.1865\n",
      "Epoch:14 | Iter:   60 | Time: 00:14:14 | Train_A Loss: 0.1767 | Train_B Loss: 0.2001\n",
      "\n",
      " EPOCH 14/100 \t Avg. Train_A loss this Epoch 0.18764357268810272 \t Avg. Train_B loss this Epoch 0.18870773911476135 \t Test loss A 0.08169781416654587 \t Test loss B 0.08197465538978577\n",
      "Epoch:15 | Iter:    0 | Time: 00:14:39 | Train_A Loss: 0.1682 | Train_B Loss: 0.1828\n",
      "Epoch:15 | Iter:   20 | Time: 00:14:52 | Train_A Loss: 0.1738 | Train_B Loss: 0.1799\n",
      "Epoch:15 | Iter:   40 | Time: 00:15:05 | Train_A Loss: 0.1772 | Train_B Loss: 0.1742\n",
      "Epoch:15 | Iter:   60 | Time: 00:15:18 | Train_A Loss: 0.1825 | Train_B Loss: 0.1825\n",
      "\n",
      " EPOCH 15/100 \t Avg. Train_A loss this Epoch 0.18662796914577484 \t Avg. Train_B loss this Epoch 0.18643447756767273 \t Test loss A 0.08039578795433044 \t Test loss B 0.07854598760604858\n",
      "Epoch:16 | Iter:    0 | Time: 00:15:42 | Train_A Loss: 0.1934 | Train_B Loss: 0.1752\n",
      "Epoch:16 | Iter:   20 | Time: 00:15:55 | Train_A Loss: 0.1930 | Train_B Loss: 0.2010\n",
      "Epoch:16 | Iter:   40 | Time: 00:16:08 | Train_A Loss: 0.1854 | Train_B Loss: 0.1937\n",
      "Epoch:16 | Iter:   60 | Time: 00:16:21 | Train_A Loss: 0.1964 | Train_B Loss: 0.1879\n",
      "\n",
      " EPOCH 16/100 \t Avg. Train_A loss this Epoch 0.18319536745548248 \t Avg. Train_B loss this Epoch 0.18400514125823975 \t Test loss A 0.08355031907558441 \t Test loss B 0.08447474241256714\n",
      "Epoch:17 | Iter:    0 | Time: 00:16:45 | Train_A Loss: 0.1720 | Train_B Loss: 0.1829\n",
      "Epoch:17 | Iter:   20 | Time: 00:16:58 | Train_A Loss: 0.1737 | Train_B Loss: 0.1869\n",
      "Epoch:17 | Iter:   40 | Time: 00:17:11 | Train_A Loss: 0.1783 | Train_B Loss: 0.1828\n",
      "Epoch:17 | Iter:   60 | Time: 00:17:24 | Train_A Loss: 0.1836 | Train_B Loss: 0.1825\n",
      "\n",
      " EPOCH 17/100 \t Avg. Train_A loss this Epoch 0.18247193098068237 \t Avg. Train_B loss this Epoch 0.18279501795768738 \t Test loss A 0.0858469009399414 \t Test loss B 0.08819889277219772\n",
      "Epoch:18 | Iter:    0 | Time: 00:17:48 | Train_A Loss: 0.1679 | Train_B Loss: 0.1753\n",
      "Epoch:18 | Iter:   20 | Time: 00:18:01 | Train_A Loss: 0.1977 | Train_B Loss: 0.1945\n",
      "Epoch:18 | Iter:   40 | Time: 00:18:14 | Train_A Loss: 0.1735 | Train_B Loss: 0.1818\n",
      "Epoch:18 | Iter:   60 | Time: 00:18:27 | Train_A Loss: 0.1756 | Train_B Loss: 0.1741\n",
      "\n",
      " EPOCH 18/100 \t Avg. Train_A loss this Epoch 0.18146143853664398 \t Avg. Train_B loss this Epoch 0.18102703988552094 \t Test loss A 0.08964221179485321 \t Test loss B 0.08905505388975143\n",
      "Epoch:19 | Iter:    0 | Time: 00:18:52 | Train_A Loss: 0.1717 | Train_B Loss: 0.1779\n",
      "Epoch:19 | Iter:   20 | Time: 00:19:05 | Train_A Loss: 0.1756 | Train_B Loss: 0.1852\n",
      "Epoch:19 | Iter:   40 | Time: 00:19:17 | Train_A Loss: 0.1794 | Train_B Loss: 0.1793\n",
      "Epoch:19 | Iter:   60 | Time: 00:19:30 | Train_A Loss: 0.1878 | Train_B Loss: 0.1922\n",
      "\n",
      " EPOCH 19/100 \t Avg. Train_A loss this Epoch 0.1787637621164322 \t Avg. Train_B loss this Epoch 0.18183539807796478 \t Test loss A 0.08585098385810852 \t Test loss B 0.08844584971666336\n",
      "Epoch:20 | Iter:    0 | Time: 00:19:54 | Train_A Loss: 0.1916 | Train_B Loss: 0.1868\n",
      "Epoch:20 | Iter:   20 | Time: 00:20:07 | Train_A Loss: 0.1794 | Train_B Loss: 0.1988\n",
      "Epoch:20 | Iter:   40 | Time: 00:20:20 | Train_A Loss: 0.1929 | Train_B Loss: 0.1704\n",
      "Epoch:20 | Iter:   60 | Time: 00:20:33 | Train_A Loss: 0.1831 | Train_B Loss: 0.1730\n",
      "\n",
      " EPOCH 20/100 \t Avg. Train_A loss this Epoch 0.18113498389720917 \t Avg. Train_B loss this Epoch 0.17930476367473602 \t Test loss A 0.08557318896055222 \t Test loss B 0.08301953226327896\n",
      "Epoch:21 | Iter:    0 | Time: 00:20:57 | Train_A Loss: 0.1807 | Train_B Loss: 0.1805\n",
      "Epoch:21 | Iter:   20 | Time: 00:21:10 | Train_A Loss: 0.1834 | Train_B Loss: 0.1894\n",
      "Epoch:21 | Iter:   40 | Time: 00:21:23 | Train_A Loss: 0.2006 | Train_B Loss: 0.1796\n",
      "Epoch:21 | Iter:   60 | Time: 00:21:36 | Train_A Loss: 0.1819 | Train_B Loss: 0.1858\n",
      "\n",
      " EPOCH 21/100 \t Avg. Train_A loss this Epoch 0.17723867297172546 \t Avg. Train_B loss this Epoch 0.17729635536670685 \t Test loss A 0.08674104511737823 \t Test loss B 0.08634059131145477\n",
      "Epoch:22 | Iter:    0 | Time: 00:22:00 | Train_A Loss: 0.1759 | Train_B Loss: 0.1704\n",
      "Epoch:22 | Iter:   20 | Time: 00:22:13 | Train_A Loss: 0.1878 | Train_B Loss: 0.1686\n",
      "Epoch:22 | Iter:   40 | Time: 00:22:26 | Train_A Loss: 0.1800 | Train_B Loss: 0.1789\n",
      "Epoch:22 | Iter:   60 | Time: 00:22:39 | Train_A Loss: 0.1602 | Train_B Loss: 0.1669\n",
      "\n",
      " EPOCH 22/100 \t Avg. Train_A loss this Epoch 0.17608867585659027 \t Avg. Train_B loss this Epoch 0.1767202913761139 \t Test loss A 0.09692183881998062 \t Test loss B 0.09378199279308319\n",
      "Epoch:23 | Iter:    0 | Time: 00:23:03 | Train_A Loss: 0.1812 | Train_B Loss: 0.1869\n",
      "Epoch:23 | Iter:   20 | Time: 00:23:17 | Train_A Loss: 0.1598 | Train_B Loss: 0.1857\n",
      "Epoch:23 | Iter:   40 | Time: 00:23:30 | Train_A Loss: 0.1732 | Train_B Loss: 0.1717\n",
      "Epoch:23 | Iter:   60 | Time: 00:23:43 | Train_A Loss: 0.1631 | Train_B Loss: 0.1710\n",
      "\n",
      " EPOCH 23/100 \t Avg. Train_A loss this Epoch 0.17728757858276367 \t Avg. Train_B loss this Epoch 0.17716942727565765 \t Test loss A 0.09444954246282578 \t Test loss B 0.09472797811031342\n",
      "Epoch:24 | Iter:    0 | Time: 00:24:07 | Train_A Loss: 0.1932 | Train_B Loss: 0.1936\n",
      "Epoch:24 | Iter:   20 | Time: 00:24:20 | Train_A Loss: 0.1844 | Train_B Loss: 0.1692\n",
      "Epoch:24 | Iter:   40 | Time: 00:24:33 | Train_A Loss: 0.1683 | Train_B Loss: 0.1655\n",
      "Epoch:24 | Iter:   60 | Time: 00:24:45 | Train_A Loss: 0.1856 | Train_B Loss: 0.1740\n",
      "\n",
      " EPOCH 24/100 \t Avg. Train_A loss this Epoch 0.17830818891525269 \t Avg. Train_B loss this Epoch 0.17795366048812866 \t Test loss A 0.09261541068553925 \t Test loss B 0.09361927956342697\n",
      "Epoch:25 | Iter:    0 | Time: 00:25:10 | Train_A Loss: 0.1909 | Train_B Loss: 0.1775\n",
      "Epoch:25 | Iter:   20 | Time: 00:25:23 | Train_A Loss: 0.1791 | Train_B Loss: 0.1841\n",
      "Epoch:25 | Iter:   40 | Time: 00:25:36 | Train_A Loss: 0.1823 | Train_B Loss: 0.1860\n",
      "Epoch:25 | Iter:   60 | Time: 00:25:49 | Train_A Loss: 0.1903 | Train_B Loss: 0.1743\n",
      "\n",
      " EPOCH 25/100 \t Avg. Train_A loss this Epoch 0.17666368186473846 \t Avg. Train_B loss this Epoch 0.17889657616615295 \t Test loss A 0.09005867689847946 \t Test loss B 0.09116630256175995\n",
      "Epoch:26 | Iter:    0 | Time: 00:26:13 | Train_A Loss: 0.1580 | Train_B Loss: 0.1872\n",
      "Epoch:26 | Iter:   20 | Time: 00:26:26 | Train_A Loss: 0.1736 | Train_B Loss: 0.1804\n",
      "Epoch:26 | Iter:   40 | Time: 00:26:39 | Train_A Loss: 0.1788 | Train_B Loss: 0.1832\n",
      "Epoch:26 | Iter:   60 | Time: 00:26:52 | Train_A Loss: 0.1739 | Train_B Loss: 0.1882\n",
      "\n",
      " EPOCH 26/100 \t Avg. Train_A loss this Epoch 0.17561225593090057 \t Avg. Train_B loss this Epoch 0.17688824236392975 \t Test loss A 0.09632498770952225 \t Test loss B 0.09910073131322861\n",
      "Epoch:27 | Iter:    0 | Time: 00:27:16 | Train_A Loss: 0.1753 | Train_B Loss: 0.1780\n",
      "Epoch:27 | Iter:   20 | Time: 00:27:29 | Train_A Loss: 0.1685 | Train_B Loss: 0.1820\n",
      "Epoch:27 | Iter:   40 | Time: 00:27:42 | Train_A Loss: 0.1765 | Train_B Loss: 0.1822\n",
      "Epoch:27 | Iter:   60 | Time: 00:27:55 | Train_A Loss: 0.1658 | Train_B Loss: 0.1658\n",
      "\n",
      " EPOCH 27/100 \t Avg. Train_A loss this Epoch 0.17604844272136688 \t Avg. Train_B loss this Epoch 0.1758081167936325 \t Test loss A 0.09381110221147537 \t Test loss B 0.09661592543125153\n",
      "Epoch:28 | Iter:    0 | Time: 00:28:19 | Train_A Loss: 0.1757 | Train_B Loss: 0.1904\n",
      "Epoch:28 | Iter:   20 | Time: 00:28:32 | Train_A Loss: 0.1714 | Train_B Loss: 0.1600\n",
      "Epoch:28 | Iter:   40 | Time: 00:28:45 | Train_A Loss: 0.1717 | Train_B Loss: 0.1708\n",
      "Epoch:28 | Iter:   60 | Time: 00:28:58 | Train_A Loss: 0.1657 | Train_B Loss: 0.1699\n",
      "\n",
      " EPOCH 28/100 \t Avg. Train_A loss this Epoch 0.17429283261299133 \t Avg. Train_B loss this Epoch 0.17468304932117462 \t Test loss A 0.0965505838394165 \t Test loss B 0.09254652261734009\n",
      "Epoch:29 | Iter:    0 | Time: 00:29:22 | Train_A Loss: 0.1671 | Train_B Loss: 0.1759\n",
      "Epoch:29 | Iter:   20 | Time: 00:29:35 | Train_A Loss: 0.1893 | Train_B Loss: 0.1783\n",
      "Epoch:29 | Iter:   40 | Time: 00:29:48 | Train_A Loss: 0.1674 | Train_B Loss: 0.1597\n",
      "Epoch:29 | Iter:   60 | Time: 00:30:01 | Train_A Loss: 0.1731 | Train_B Loss: 0.1814\n",
      "\n",
      " EPOCH 29/100 \t Avg. Train_A loss this Epoch 0.17387719452381134 \t Avg. Train_B loss this Epoch 0.17498821020126343 \t Test loss A 0.09656422585248947 \t Test loss B 0.09777761250734329\n",
      "Epoch:30 | Iter:    0 | Time: 00:30:25 | Train_A Loss: 0.1617 | Train_B Loss: 0.1788\n",
      "Epoch:30 | Iter:   20 | Time: 00:30:38 | Train_A Loss: 0.1852 | Train_B Loss: 0.1707\n",
      "Epoch:30 | Iter:   40 | Time: 00:30:51 | Train_A Loss: 0.1650 | Train_B Loss: 0.1693\n",
      "Epoch:30 | Iter:   60 | Time: 00:31:04 | Train_A Loss: 0.1531 | Train_B Loss: 0.1796\n",
      "\n",
      " EPOCH 30/100 \t Avg. Train_A loss this Epoch 0.17433108389377594 \t Avg. Train_B loss this Epoch 0.17487728595733643 \t Test loss A 0.09093572199344635 \t Test loss B 0.10125993192195892\n",
      "Epoch:31 | Iter:    0 | Time: 00:31:28 | Train_A Loss: 0.1677 | Train_B Loss: 0.1765\n",
      "Epoch:31 | Iter:   20 | Time: 00:31:41 | Train_A Loss: 0.1868 | Train_B Loss: 0.1618\n",
      "Epoch:31 | Iter:   40 | Time: 00:31:54 | Train_A Loss: 0.1852 | Train_B Loss: 0.1906\n",
      "Epoch:31 | Iter:   60 | Time: 00:32:06 | Train_A Loss: 0.1639 | Train_B Loss: 0.1694\n",
      "\n",
      " EPOCH 31/100 \t Avg. Train_A loss this Epoch 0.17460155487060547 \t Avg. Train_B loss this Epoch 0.17673292756080627 \t Test loss A 0.09485053271055222 \t Test loss B 0.09750834852457047\n",
      "Epoch:32 | Iter:    0 | Time: 00:32:31 | Train_A Loss: 0.1694 | Train_B Loss: 0.1720\n",
      "Epoch:32 | Iter:   20 | Time: 00:32:44 | Train_A Loss: 0.1695 | Train_B Loss: 0.1901\n",
      "Epoch:32 | Iter:   40 | Time: 00:32:57 | Train_A Loss: 0.1604 | Train_B Loss: 0.1802\n",
      "Epoch:32 | Iter:   60 | Time: 00:33:09 | Train_A Loss: 0.1608 | Train_B Loss: 0.1676\n",
      "\n",
      " EPOCH 32/100 \t Avg. Train_A loss this Epoch 0.1708512306213379 \t Avg. Train_B loss this Epoch 0.17456813156604767 \t Test loss A 0.08656422793865204 \t Test loss B 0.09045999497175217\n",
      "Epoch:33 | Iter:    0 | Time: 00:33:34 | Train_A Loss: 0.1754 | Train_B Loss: 0.1641\n",
      "Epoch:33 | Iter:   20 | Time: 00:33:47 | Train_A Loss: 0.1859 | Train_B Loss: 0.1716\n",
      "Epoch:33 | Iter:   40 | Time: 00:34:00 | Train_A Loss: 0.1796 | Train_B Loss: 0.1621\n",
      "Epoch:33 | Iter:   60 | Time: 00:34:13 | Train_A Loss: 0.1644 | Train_B Loss: 0.1618\n",
      "\n",
      " EPOCH 33/100 \t Avg. Train_A loss this Epoch 0.17441241443157196 \t Avg. Train_B loss this Epoch 0.17336110770702362 \t Test loss A 0.0967249721288681 \t Test loss B 0.10082490742206573\n",
      "Epoch:34 | Iter:    0 | Time: 00:34:37 | Train_A Loss: 0.1637 | Train_B Loss: 0.1551\n",
      "Epoch:34 | Iter:   20 | Time: 00:34:50 | Train_A Loss: 0.1530 | Train_B Loss: 0.1608\n",
      "Epoch:34 | Iter:   40 | Time: 00:35:03 | Train_A Loss: 0.1818 | Train_B Loss: 0.1949\n",
      "Epoch:34 | Iter:   60 | Time: 00:35:16 | Train_A Loss: 0.1797 | Train_B Loss: 0.1845\n",
      "\n",
      " EPOCH 34/100 \t Avg. Train_A loss this Epoch 0.17415568232536316 \t Avg. Train_B loss this Epoch 0.17365996539592743 \t Test loss A 0.09635703265666962 \t Test loss B 0.09588869661092758\n",
      "Epoch:35 | Iter:    0 | Time: 00:35:40 | Train_A Loss: 0.1666 | Train_B Loss: 0.1562\n",
      "Epoch:35 | Iter:   20 | Time: 00:35:53 | Train_A Loss: 0.1708 | Train_B Loss: 0.1641\n",
      "Epoch:35 | Iter:   40 | Time: 00:36:05 | Train_A Loss: 0.1581 | Train_B Loss: 0.1766\n",
      "Epoch:35 | Iter:   60 | Time: 00:36:18 | Train_A Loss: 0.1922 | Train_B Loss: 0.1817\n",
      "\n",
      " EPOCH 35/100 \t Avg. Train_A loss this Epoch 0.17384526133537292 \t Avg. Train_B loss this Epoch 0.1727353185415268 \t Test loss A 0.09625128656625748 \t Test loss B 0.09702826291322708\n",
      "Epoch:36 | Iter:    0 | Time: 00:36:42 | Train_A Loss: 0.1812 | Train_B Loss: 0.1734\n",
      "Epoch:36 | Iter:   20 | Time: 00:36:55 | Train_A Loss: 0.1776 | Train_B Loss: 0.1691\n",
      "Epoch:36 | Iter:   40 | Time: 00:37:08 | Train_A Loss: 0.1825 | Train_B Loss: 0.1771\n",
      "Epoch:36 | Iter:   60 | Time: 00:37:21 | Train_A Loss: 0.1785 | Train_B Loss: 0.1747\n",
      "\n",
      " EPOCH 36/100 \t Avg. Train_A loss this Epoch 0.17549513280391693 \t Avg. Train_B loss this Epoch 0.17090336978435516 \t Test loss A 0.10029669851064682 \t Test loss B 0.1026250571012497\n",
      "Epoch:37 | Iter:    0 | Time: 00:37:45 | Train_A Loss: 0.1836 | Train_B Loss: 0.2001\n",
      "Epoch:37 | Iter:   20 | Time: 00:37:58 | Train_A Loss: 0.1722 | Train_B Loss: 0.1690\n",
      "Epoch:37 | Iter:   40 | Time: 00:38:11 | Train_A Loss: 0.1841 | Train_B Loss: 0.1772\n",
      "Epoch:37 | Iter:   60 | Time: 00:38:24 | Train_A Loss: 0.1662 | Train_B Loss: 0.1831\n",
      "\n",
      " EPOCH 37/100 \t Avg. Train_A loss this Epoch 0.17255306243896484 \t Avg. Train_B loss this Epoch 0.17431382834911346 \t Test loss A 0.09386324882507324 \t Test loss B 0.09430356323719025\n",
      "Epoch:38 | Iter:    0 | Time: 00:38:48 | Train_A Loss: 0.1716 | Train_B Loss: 0.1569\n",
      "Epoch:38 | Iter:   20 | Time: 00:39:01 | Train_A Loss: 0.1629 | Train_B Loss: 0.1725\n",
      "Epoch:38 | Iter:   40 | Time: 00:39:14 | Train_A Loss: 0.1721 | Train_B Loss: 0.1750\n",
      "Epoch:38 | Iter:   60 | Time: 00:39:26 | Train_A Loss: 0.1705 | Train_B Loss: 0.1724\n",
      "\n",
      " EPOCH 38/100 \t Avg. Train_A loss this Epoch 0.17312602698802948 \t Avg. Train_B loss this Epoch 0.17390486598014832 \t Test loss A 0.09842433035373688 \t Test loss B 0.09944789856672287\n",
      "Epoch:39 | Iter:    0 | Time: 00:39:51 | Train_A Loss: 0.1728 | Train_B Loss: 0.1629\n",
      "Epoch:39 | Iter:   20 | Time: 00:40:04 | Train_A Loss: 0.1655 | Train_B Loss: 0.1557\n",
      "Epoch:39 | Iter:   40 | Time: 00:40:17 | Train_A Loss: 0.1777 | Train_B Loss: 0.1503\n",
      "Epoch:39 | Iter:   60 | Time: 00:40:30 | Train_A Loss: 0.1727 | Train_B Loss: 0.1842\n",
      "\n",
      " EPOCH 39/100 \t Avg. Train_A loss this Epoch 0.17308393120765686 \t Avg. Train_B loss this Epoch 0.1723107546567917 \t Test loss A 0.10051870346069336 \t Test loss B 0.1013813465833664\n",
      "Epoch:40 | Iter:    0 | Time: 00:40:54 | Train_A Loss: 0.1655 | Train_B Loss: 0.1915\n",
      "Epoch:40 | Iter:   20 | Time: 00:41:07 | Train_A Loss: 0.1695 | Train_B Loss: 0.1666\n",
      "Epoch:40 | Iter:   40 | Time: 00:41:20 | Train_A Loss: 0.1516 | Train_B Loss: 0.1579\n",
      "Epoch:40 | Iter:   60 | Time: 00:41:33 | Train_A Loss: 0.1786 | Train_B Loss: 0.1982\n",
      "\n",
      " EPOCH 40/100 \t Avg. Train_A loss this Epoch 0.17081962525844574 \t Avg. Train_B loss this Epoch 0.17368152737617493 \t Test loss A 0.10302555561065674 \t Test loss B 0.1022944301366806\n",
      "Epoch:41 | Iter:    0 | Time: 00:41:57 | Train_A Loss: 0.1696 | Train_B Loss: 0.1636\n",
      "Epoch:41 | Iter:   20 | Time: 00:42:10 | Train_A Loss: 0.1614 | Train_B Loss: 0.1546\n",
      "Epoch:41 | Iter:   40 | Time: 00:42:23 | Train_A Loss: 0.1685 | Train_B Loss: 0.1864\n",
      "Epoch:41 | Iter:   60 | Time: 00:42:36 | Train_A Loss: 0.1646 | Train_B Loss: 0.1573\n",
      "\n",
      " EPOCH 41/100 \t Avg. Train_A loss this Epoch 0.17275121808052063 \t Avg. Train_B loss this Epoch 0.17290787398815155 \t Test loss A 0.0969398021697998 \t Test loss B 0.10306801646947861\n",
      "Epoch:42 | Iter:    0 | Time: 00:43:00 | Train_A Loss: 0.1868 | Train_B Loss: 0.1761\n",
      "Epoch:42 | Iter:   20 | Time: 00:43:13 | Train_A Loss: 0.1608 | Train_B Loss: 0.1572\n",
      "Epoch:42 | Iter:   40 | Time: 00:43:25 | Train_A Loss: 0.1737 | Train_B Loss: 0.1773\n",
      "Epoch:42 | Iter:   60 | Time: 00:43:38 | Train_A Loss: 0.1706 | Train_B Loss: 0.1778\n",
      "\n",
      " EPOCH 42/100 \t Avg. Train_A loss this Epoch 0.17355774343013763 \t Avg. Train_B loss this Epoch 0.1712312549352646 \t Test loss A 0.10077208280563354 \t Test loss B 0.09798973798751831\n",
      "Epoch:43 | Iter:    0 | Time: 00:44:02 | Train_A Loss: 0.1821 | Train_B Loss: 0.1626\n",
      "Epoch:43 | Iter:   20 | Time: 00:44:15 | Train_A Loss: 0.1644 | Train_B Loss: 0.1746\n",
      "Epoch:43 | Iter:   40 | Time: 00:44:28 | Train_A Loss: 0.1775 | Train_B Loss: 0.1765\n",
      "Epoch:43 | Iter:   60 | Time: 00:44:41 | Train_A Loss: 0.1784 | Train_B Loss: 0.1903\n",
      "\n",
      " EPOCH 43/100 \t Avg. Train_A loss this Epoch 0.17350506782531738 \t Avg. Train_B loss this Epoch 0.17335888743400574 \t Test loss A 0.0940917357802391 \t Test loss B 0.09584531933069229\n",
      "Epoch:44 | Iter:    0 | Time: 00:45:05 | Train_A Loss: 0.1592 | Train_B Loss: 0.1850\n",
      "Epoch:44 | Iter:   20 | Time: 00:45:18 | Train_A Loss: 0.1771 | Train_B Loss: 0.1889\n",
      "Epoch:44 | Iter:   40 | Time: 00:45:31 | Train_A Loss: 0.1749 | Train_B Loss: 0.1675\n",
      "Epoch:44 | Iter:   60 | Time: 00:45:44 | Train_A Loss: 0.1730 | Train_B Loss: 0.1766\n",
      "\n",
      " EPOCH 44/100 \t Avg. Train_A loss this Epoch 0.17315971851348877 \t Avg. Train_B loss this Epoch 0.17358161509037018 \t Test loss A 0.09317567199468613 \t Test loss B 0.09615572541952133\n",
      "Epoch:45 | Iter:    0 | Time: 00:46:08 | Train_A Loss: 0.1753 | Train_B Loss: 0.1701\n",
      "Epoch:45 | Iter:   20 | Time: 00:46:21 | Train_A Loss: 0.1614 | Train_B Loss: 0.1693\n",
      "Epoch:45 | Iter:   40 | Time: 00:46:34 | Train_A Loss: 0.1670 | Train_B Loss: 0.1805\n",
      "Epoch:45 | Iter:   60 | Time: 00:46:47 | Train_A Loss: 0.1728 | Train_B Loss: 0.1719\n",
      "\n",
      " EPOCH 45/100 \t Avg. Train_A loss this Epoch 0.1733669489622116 \t Avg. Train_B loss this Epoch 0.171960711479187 \t Test loss A 0.10042314976453781 \t Test loss B 0.09646504372358322\n",
      "Epoch:46 | Iter:    0 | Time: 00:47:11 | Train_A Loss: 0.1827 | Train_B Loss: 0.1677\n",
      "Epoch:46 | Iter:   20 | Time: 00:47:24 | Train_A Loss: 0.1689 | Train_B Loss: 0.1662\n",
      "Epoch:46 | Iter:   40 | Time: 00:47:37 | Train_A Loss: 0.1639 | Train_B Loss: 0.1888\n",
      "Epoch:46 | Iter:   60 | Time: 00:47:50 | Train_A Loss: 0.1755 | Train_B Loss: 0.1953\n",
      "\n",
      " EPOCH 46/100 \t Avg. Train_A loss this Epoch 0.17159777879714966 \t Avg. Train_B loss this Epoch 0.17289412021636963 \t Test loss A 0.0954819917678833 \t Test loss B 0.09730443358421326\n",
      "Epoch:47 | Iter:    0 | Time: 00:48:15 | Train_A Loss: 0.1660 | Train_B Loss: 0.1781\n",
      "Epoch:47 | Iter:   20 | Time: 00:48:28 | Train_A Loss: 0.1787 | Train_B Loss: 0.1663\n",
      "Epoch:47 | Iter:   40 | Time: 00:48:41 | Train_A Loss: 0.1617 | Train_B Loss: 0.1643\n",
      "Epoch:47 | Iter:   60 | Time: 00:48:54 | Train_A Loss: 0.1765 | Train_B Loss: 0.1784\n",
      "\n",
      " EPOCH 47/100 \t Avg. Train_A loss this Epoch 0.1731491982936859 \t Avg. Train_B loss this Epoch 0.17324136197566986 \t Test loss A 0.09153873473405838 \t Test loss B 0.0920029804110527\n",
      "Epoch:48 | Iter:    0 | Time: 00:49:18 | Train_A Loss: 0.1717 | Train_B Loss: 0.1595\n",
      "Epoch:48 | Iter:   20 | Time: 00:49:31 | Train_A Loss: 0.1656 | Train_B Loss: 0.1773\n",
      "Epoch:48 | Iter:   40 | Time: 00:49:44 | Train_A Loss: 0.1739 | Train_B Loss: 0.1640\n",
      "Epoch:48 | Iter:   60 | Time: 00:49:57 | Train_A Loss: 0.1720 | Train_B Loss: 0.1815\n",
      "\n",
      " EPOCH 48/100 \t Avg. Train_A loss this Epoch 0.1720786690711975 \t Avg. Train_B loss this Epoch 0.1712469607591629 \t Test loss A 0.09271504729986191 \t Test loss B 0.09257987141609192\n",
      "Epoch:49 | Iter:    0 | Time: 00:50:21 | Train_A Loss: 0.1667 | Train_B Loss: 0.1741\n",
      "Epoch:49 | Iter:   20 | Time: 00:50:34 | Train_A Loss: 0.1786 | Train_B Loss: 0.1866\n",
      "Epoch:49 | Iter:   40 | Time: 00:50:47 | Train_A Loss: 0.1648 | Train_B Loss: 0.1707\n",
      "Epoch:49 | Iter:   60 | Time: 00:51:00 | Train_A Loss: 0.1697 | Train_B Loss: 0.1699\n",
      "\n",
      " EPOCH 49/100 \t Avg. Train_A loss this Epoch 0.17174503207206726 \t Avg. Train_B loss this Epoch 0.1741505116224289 \t Test loss A 0.09222470968961716 \t Test loss B 0.0928524062037468\n",
      "Epoch:50 | Iter:    0 | Time: 00:51:24 | Train_A Loss: 0.1589 | Train_B Loss: 0.1760\n",
      "Epoch:50 | Iter:   20 | Time: 00:51:37 | Train_A Loss: 0.1752 | Train_B Loss: 0.1547\n",
      "Epoch:50 | Iter:   40 | Time: 00:51:50 | Train_A Loss: 0.1742 | Train_B Loss: 0.1659\n",
      "Epoch:50 | Iter:   60 | Time: 00:52:03 | Train_A Loss: 0.1791 | Train_B Loss: 0.1779\n",
      "\n",
      " EPOCH 50/100 \t Avg. Train_A loss this Epoch 0.17141173779964447 \t Avg. Train_B loss this Epoch 0.17254869639873505 \t Test loss A 0.09626832604408264 \t Test loss B 0.09977888315916061\n",
      "Epoch:51 | Iter:    0 | Time: 00:52:27 | Train_A Loss: 0.1742 | Train_B Loss: 0.1868\n",
      "Epoch:51 | Iter:   20 | Time: 00:52:40 | Train_A Loss: 0.1741 | Train_B Loss: 0.1632\n",
      "Epoch:51 | Iter:   40 | Time: 00:52:53 | Train_A Loss: 0.1630 | Train_B Loss: 0.1665\n",
      "Epoch:51 | Iter:   60 | Time: 00:53:06 | Train_A Loss: 0.1758 | Train_B Loss: 0.1690\n",
      "\n",
      " EPOCH 51/100 \t Avg. Train_A loss this Epoch 0.17431850731372833 \t Avg. Train_B loss this Epoch 0.17396949231624603 \t Test loss A 0.10179831087589264 \t Test loss B 0.1021040752530098\n",
      "Epoch:52 | Iter:    0 | Time: 00:53:31 | Train_A Loss: 0.1835 | Train_B Loss: 0.1612\n",
      "Epoch:52 | Iter:   20 | Time: 00:53:44 | Train_A Loss: 0.1796 | Train_B Loss: 0.1637\n",
      "Epoch:52 | Iter:   40 | Time: 00:53:57 | Train_A Loss: 0.1661 | Train_B Loss: 0.1762\n",
      "Epoch:52 | Iter:   60 | Time: 00:54:10 | Train_A Loss: 0.1663 | Train_B Loss: 0.1569\n",
      "\n",
      " EPOCH 52/100 \t Avg. Train_A loss this Epoch 0.17202098667621613 \t Avg. Train_B loss this Epoch 0.1711936891078949 \t Test loss A 0.09905734658241272 \t Test loss B 0.09355068951845169\n",
      "Epoch:53 | Iter:    0 | Time: 00:54:34 | Train_A Loss: 0.1697 | Train_B Loss: 0.1896\n",
      "Epoch:53 | Iter:   20 | Time: 00:54:47 | Train_A Loss: 0.1860 | Train_B Loss: 0.1669\n",
      "Epoch:53 | Iter:   40 | Time: 00:55:00 | Train_A Loss: 0.1915 | Train_B Loss: 0.1824\n",
      "Epoch:53 | Iter:   60 | Time: 00:55:13 | Train_A Loss: 0.1632 | Train_B Loss: 0.1600\n",
      "\n",
      " EPOCH 53/100 \t Avg. Train_A loss this Epoch 0.17183923721313477 \t Avg. Train_B loss this Epoch 0.1736510992050171 \t Test loss A 0.09800903499126434 \t Test loss B 0.10208909958600998\n",
      "Epoch:54 | Iter:    0 | Time: 00:55:37 | Train_A Loss: 0.1725 | Train_B Loss: 0.1849\n",
      "Epoch:54 | Iter:   20 | Time: 00:55:50 | Train_A Loss: 0.1688 | Train_B Loss: 0.1725\n",
      "Epoch:54 | Iter:   40 | Time: 00:56:03 | Train_A Loss: 0.1625 | Train_B Loss: 0.1834\n",
      "Epoch:54 | Iter:   60 | Time: 00:56:16 | Train_A Loss: 0.1639 | Train_B Loss: 0.1767\n",
      "\n",
      " EPOCH 54/100 \t Avg. Train_A loss this Epoch 0.16960933804512024 \t Avg. Train_B loss this Epoch 0.1725277602672577 \t Test loss A 0.09682638198137283 \t Test loss B 0.09627839922904968\n",
      "Epoch:55 | Iter:    0 | Time: 00:56:40 | Train_A Loss: 0.1712 | Train_B Loss: 0.1788\n",
      "Epoch:55 | Iter:   20 | Time: 00:56:53 | Train_A Loss: 0.1727 | Train_B Loss: 0.1781\n",
      "Epoch:55 | Iter:   40 | Time: 00:57:06 | Train_A Loss: 0.1748 | Train_B Loss: 0.1832\n",
      "Epoch:55 | Iter:   60 | Time: 00:57:19 | Train_A Loss: 0.1620 | Train_B Loss: 0.1568\n",
      "\n",
      " EPOCH 55/100 \t Avg. Train_A loss this Epoch 0.1711597591638565 \t Avg. Train_B loss this Epoch 0.17171862721443176 \t Test loss A 0.09918347746133804 \t Test loss B 0.09790872037410736\n",
      "Epoch:56 | Iter:    0 | Time: 00:57:43 | Train_A Loss: 0.1732 | Train_B Loss: 0.1564\n",
      "Epoch:56 | Iter:   20 | Time: 00:57:56 | Train_A Loss: 0.1713 | Train_B Loss: 0.1749\n",
      "Epoch:56 | Iter:   40 | Time: 00:58:09 | Train_A Loss: 0.1781 | Train_B Loss: 0.1629\n",
      "Epoch:56 | Iter:   60 | Time: 00:58:22 | Train_A Loss: 0.1793 | Train_B Loss: 0.1662\n",
      "\n",
      " EPOCH 56/100 \t Avg. Train_A loss this Epoch 0.17309200763702393 \t Avg. Train_B loss this Epoch 0.17190119624137878 \t Test loss A 0.09423939883708954 \t Test loss B 0.09773556888103485\n",
      "Epoch:57 | Iter:    0 | Time: 00:58:46 | Train_A Loss: 0.1644 | Train_B Loss: 0.1724\n",
      "Epoch:57 | Iter:   20 | Time: 00:58:59 | Train_A Loss: 0.1727 | Train_B Loss: 0.1739\n",
      "Epoch:57 | Iter:   40 | Time: 00:59:12 | Train_A Loss: 0.1737 | Train_B Loss: 0.1630\n",
      "Epoch:57 | Iter:   60 | Time: 00:59:25 | Train_A Loss: 0.1777 | Train_B Loss: 0.1716\n",
      "\n",
      " EPOCH 57/100 \t Avg. Train_A loss this Epoch 0.17063084244728088 \t Avg. Train_B loss this Epoch 0.17414981126785278 \t Test loss A 0.10197821259498596 \t Test loss B 0.09920623898506165\n",
      "Epoch:58 | Iter:    0 | Time: 00:59:49 | Train_A Loss: 0.1753 | Train_B Loss: 0.1789\n",
      "Epoch:58 | Iter:   20 | Time: 01:00:02 | Train_A Loss: 0.1760 | Train_B Loss: 0.1805\n",
      "Epoch:58 | Iter:   40 | Time: 01:00:15 | Train_A Loss: 0.1517 | Train_B Loss: 0.1766\n",
      "Epoch:58 | Iter:   60 | Time: 01:00:28 | Train_A Loss: 0.1621 | Train_B Loss: 0.1730\n",
      "\n",
      " EPOCH 58/100 \t Avg. Train_A loss this Epoch 0.17227542400360107 \t Avg. Train_B loss this Epoch 0.17306961119174957 \t Test loss A 0.09968055039644241 \t Test loss B 0.09669708460569382\n",
      "Epoch:59 | Iter:    0 | Time: 01:00:52 | Train_A Loss: 0.1514 | Train_B Loss: 0.1644\n",
      "Epoch:59 | Iter:   20 | Time: 01:01:05 | Train_A Loss: 0.1789 | Train_B Loss: 0.1884\n",
      "Epoch:59 | Iter:   40 | Time: 01:01:18 | Train_A Loss: 0.1693 | Train_B Loss: 0.1726\n",
      "Epoch:59 | Iter:   60 | Time: 01:01:31 | Train_A Loss: 0.1617 | Train_B Loss: 0.1752\n",
      "\n",
      " EPOCH 59/100 \t Avg. Train_A loss this Epoch 0.17231401801109314 \t Avg. Train_B loss this Epoch 0.1697765290737152 \t Test loss A 0.1006760373711586 \t Test loss B 0.10360481590032578\n",
      "Epoch:60 | Iter:    0 | Time: 01:01:55 | Train_A Loss: 0.1781 | Train_B Loss: 0.1758\n",
      "Epoch:60 | Iter:   20 | Time: 01:02:08 | Train_A Loss: 0.1729 | Train_B Loss: 0.1649\n",
      "Epoch:60 | Iter:   40 | Time: 01:02:21 | Train_A Loss: 0.1864 | Train_B Loss: 0.1573\n",
      "Epoch:60 | Iter:   60 | Time: 01:02:34 | Train_A Loss: 0.1701 | Train_B Loss: 0.1843\n",
      "\n",
      " EPOCH 60/100 \t Avg. Train_A loss this Epoch 0.17041191458702087 \t Avg. Train_B loss this Epoch 0.17016974091529846 \t Test loss A 0.09309619665145874 \t Test loss B 0.09647683799266815\n",
      "Epoch:61 | Iter:    0 | Time: 01:02:58 | Train_A Loss: 0.1886 | Train_B Loss: 0.1686\n",
      "Epoch:61 | Iter:   20 | Time: 01:03:11 | Train_A Loss: 0.1768 | Train_B Loss: 0.1826\n",
      "Epoch:61 | Iter:   40 | Time: 01:03:24 | Train_A Loss: 0.1887 | Train_B Loss: 0.1786\n",
      "Epoch:61 | Iter:   60 | Time: 01:03:37 | Train_A Loss: 0.1729 | Train_B Loss: 0.1633\n",
      "\n",
      " EPOCH 61/100 \t Avg. Train_A loss this Epoch 0.17244145274162292 \t Avg. Train_B loss this Epoch 0.1724252998828888 \t Test loss A 0.09789150953292847 \t Test loss B 0.09610329568386078\n",
      "Epoch:62 | Iter:    0 | Time: 01:04:01 | Train_A Loss: 0.1698 | Train_B Loss: 0.1641\n",
      "Epoch:62 | Iter:   20 | Time: 01:04:14 | Train_A Loss: 0.1711 | Train_B Loss: 0.1792\n",
      "Epoch:62 | Iter:   40 | Time: 01:04:27 | Train_A Loss: 0.1678 | Train_B Loss: 0.1368\n",
      "Epoch:62 | Iter:   60 | Time: 01:04:40 | Train_A Loss: 0.1755 | Train_B Loss: 0.1732\n",
      "\n",
      " EPOCH 62/100 \t Avg. Train_A loss this Epoch 0.1712263971567154 \t Avg. Train_B loss this Epoch 0.17338496446609497 \t Test loss A 0.09763488918542862 \t Test loss B 0.09959414601325989\n",
      "Epoch:63 | Iter:    0 | Time: 01:05:04 | Train_A Loss: 0.1711 | Train_B Loss: 0.1712\n",
      "Epoch:63 | Iter:   20 | Time: 01:05:17 | Train_A Loss: 0.1612 | Train_B Loss: 0.1648\n",
      "Epoch:63 | Iter:   40 | Time: 01:05:30 | Train_A Loss: 0.1781 | Train_B Loss: 0.1841\n",
      "Epoch:63 | Iter:   60 | Time: 01:05:43 | Train_A Loss: 0.1721 | Train_B Loss: 0.1850\n",
      "\n",
      " EPOCH 63/100 \t Avg. Train_A loss this Epoch 0.17173582315444946 \t Avg. Train_B loss this Epoch 0.17100949585437775 \t Test loss A 0.09847670793533325 \t Test loss B 0.09665166586637497\n",
      "Epoch:64 | Iter:    0 | Time: 01:06:07 | Train_A Loss: 0.1698 | Train_B Loss: 0.1628\n",
      "Epoch:64 | Iter:   20 | Time: 01:06:20 | Train_A Loss: 0.1857 | Train_B Loss: 0.1741\n",
      "Epoch:64 | Iter:   40 | Time: 01:06:33 | Train_A Loss: 0.1648 | Train_B Loss: 0.1850\n",
      "Epoch:64 | Iter:   60 | Time: 01:06:46 | Train_A Loss: 0.1806 | Train_B Loss: 0.1782\n",
      "\n",
      " EPOCH 64/100 \t Avg. Train_A loss this Epoch 0.17217044532299042 \t Avg. Train_B loss this Epoch 0.17196276783943176 \t Test loss A 0.09023158252239227 \t Test loss B 0.09169111400842667\n",
      "Epoch:65 | Iter:    0 | Time: 01:07:11 | Train_A Loss: 0.1798 | Train_B Loss: 0.1751\n",
      "Epoch:65 | Iter:   20 | Time: 01:07:24 | Train_A Loss: 0.1602 | Train_B Loss: 0.1548\n",
      "Epoch:65 | Iter:   40 | Time: 01:07:37 | Train_A Loss: 0.1711 | Train_B Loss: 0.1902\n",
      "Epoch:65 | Iter:   60 | Time: 01:07:50 | Train_A Loss: 0.1783 | Train_B Loss: 0.1758\n",
      "\n",
      " EPOCH 65/100 \t Avg. Train_A loss this Epoch 0.17171332240104675 \t Avg. Train_B loss this Epoch 0.17309266328811646 \t Test loss A 0.0989590659737587 \t Test loss B 0.10374348610639572\n",
      "Epoch:66 | Iter:    0 | Time: 01:08:14 | Train_A Loss: 0.1536 | Train_B Loss: 0.1540\n",
      "Epoch:66 | Iter:   20 | Time: 01:08:27 | Train_A Loss: 0.1787 | Train_B Loss: 0.1869\n",
      "Epoch:66 | Iter:   40 | Time: 01:08:40 | Train_A Loss: 0.1873 | Train_B Loss: 0.1943\n",
      "Epoch:66 | Iter:   60 | Time: 01:08:53 | Train_A Loss: 0.1762 | Train_B Loss: 0.1884\n",
      "\n",
      " EPOCH 66/100 \t Avg. Train_A loss this Epoch 0.17196893692016602 \t Avg. Train_B loss this Epoch 0.17232878506183624 \t Test loss A 0.09784533828496933 \t Test loss B 0.09915561228990555\n",
      "Epoch:67 | Iter:    0 | Time: 01:09:18 | Train_A Loss: 0.1633 | Train_B Loss: 0.1760\n",
      "Epoch:67 | Iter:   20 | Time: 01:09:31 | Train_A Loss: 0.1648 | Train_B Loss: 0.1638\n",
      "Epoch:67 | Iter:   40 | Time: 01:09:44 | Train_A Loss: 0.1718 | Train_B Loss: 0.1645\n",
      "Epoch:67 | Iter:   60 | Time: 01:09:57 | Train_A Loss: 0.1754 | Train_B Loss: 0.1837\n",
      "\n",
      " EPOCH 67/100 \t Avg. Train_A loss this Epoch 0.171493798494339 \t Avg. Train_B loss this Epoch 0.17260213196277618 \t Test loss A 0.10204523801803589 \t Test loss B 0.09698358923196793\n",
      "Epoch:68 | Iter:    0 | Time: 01:10:21 | Train_A Loss: 0.1871 | Train_B Loss: 0.1680\n",
      "Epoch:68 | Iter:   20 | Time: 01:10:35 | Train_A Loss: 0.1708 | Train_B Loss: 0.1741\n",
      "Epoch:68 | Iter:   40 | Time: 01:10:48 | Train_A Loss: 0.1727 | Train_B Loss: 0.1726\n",
      "Epoch:68 | Iter:   60 | Time: 01:11:01 | Train_A Loss: 0.1566 | Train_B Loss: 0.1622\n",
      "\n",
      " EPOCH 68/100 \t Avg. Train_A loss this Epoch 0.17062576115131378 \t Avg. Train_B loss this Epoch 0.17163461446762085 \t Test loss A 0.10228639841079712 \t Test loss B 0.09583593904972076\n",
      "Epoch:69 | Iter:    0 | Time: 01:11:25 | Train_A Loss: 0.1728 | Train_B Loss: 0.1636\n",
      "Epoch:69 | Iter:   20 | Time: 01:11:39 | Train_A Loss: 0.1752 | Train_B Loss: 0.1718\n",
      "Epoch:69 | Iter:   40 | Time: 01:11:52 | Train_A Loss: 0.1840 | Train_B Loss: 0.1741\n",
      "Epoch:69 | Iter:   60 | Time: 01:12:05 | Train_A Loss: 0.1703 | Train_B Loss: 0.1662\n",
      "\n",
      " EPOCH 69/100 \t Avg. Train_A loss this Epoch 0.17007885873317719 \t Avg. Train_B loss this Epoch 0.16921207308769226 \t Test loss A 0.09721557796001434 \t Test loss B 0.09552338719367981\n",
      "Epoch:70 | Iter:    0 | Time: 01:12:29 | Train_A Loss: 0.1889 | Train_B Loss: 0.1721\n",
      "Epoch:70 | Iter:   20 | Time: 01:12:42 | Train_A Loss: 0.1720 | Train_B Loss: 0.1690\n",
      "Epoch:70 | Iter:   40 | Time: 01:12:55 | Train_A Loss: 0.1785 | Train_B Loss: 0.1673\n",
      "Epoch:70 | Iter:   60 | Time: 01:13:08 | Train_A Loss: 0.1782 | Train_B Loss: 0.1877\n",
      "\n",
      " EPOCH 70/100 \t Avg. Train_A loss this Epoch 0.17303790152072906 \t Avg. Train_B loss this Epoch 0.1709979623556137 \t Test loss A 0.09888626635074615 \t Test loss B 0.09584714472293854\n",
      "Epoch:71 | Iter:    0 | Time: 01:13:33 | Train_A Loss: 0.1720 | Train_B Loss: 0.1635\n",
      "Epoch:71 | Iter:   20 | Time: 01:13:46 | Train_A Loss: 0.1646 | Train_B Loss: 0.1629\n",
      "Epoch:71 | Iter:   40 | Time: 01:13:59 | Train_A Loss: 0.1673 | Train_B Loss: 0.1658\n",
      "Epoch:71 | Iter:   60 | Time: 01:14:12 | Train_A Loss: 0.1667 | Train_B Loss: 0.1737\n",
      "\n",
      " EPOCH 71/100 \t Avg. Train_A loss this Epoch 0.1712033748626709 \t Avg. Train_B loss this Epoch 0.172317236661911 \t Test loss A 0.09485755860805511 \t Test loss B 0.09686745703220367\n",
      "Epoch:72 | Iter:    0 | Time: 01:14:36 | Train_A Loss: 0.1714 | Train_B Loss: 0.1686\n",
      "Epoch:72 | Iter:   20 | Time: 01:14:50 | Train_A Loss: 0.1695 | Train_B Loss: 0.1671\n",
      "Epoch:72 | Iter:   40 | Time: 01:15:03 | Train_A Loss: 0.1631 | Train_B Loss: 0.1769\n",
      "Epoch:72 | Iter:   60 | Time: 01:15:16 | Train_A Loss: 0.1757 | Train_B Loss: 0.1820\n",
      "\n",
      " EPOCH 72/100 \t Avg. Train_A loss this Epoch 0.17151449620723724 \t Avg. Train_B loss this Epoch 0.1732923537492752 \t Test loss A 0.09475970268249512 \t Test loss B 0.0950518399477005\n",
      "Epoch:73 | Iter:    0 | Time: 01:15:40 | Train_A Loss: 0.1753 | Train_B Loss: 0.1838\n",
      "Epoch:73 | Iter:   20 | Time: 01:15:53 | Train_A Loss: 0.1814 | Train_B Loss: 0.1752\n",
      "Epoch:73 | Iter:   40 | Time: 01:16:06 | Train_A Loss: 0.1801 | Train_B Loss: 0.1748\n",
      "Epoch:73 | Iter:   60 | Time: 01:16:19 | Train_A Loss: 0.1673 | Train_B Loss: 0.1792\n",
      "\n",
      " EPOCH 73/100 \t Avg. Train_A loss this Epoch 0.1711776852607727 \t Avg. Train_B loss this Epoch 0.17135408520698547 \t Test loss A 0.09627872705459595 \t Test loss B 0.09503625333309174\n",
      "Epoch:74 | Iter:    0 | Time: 01:16:44 | Train_A Loss: 0.1887 | Train_B Loss: 0.1748\n",
      "Epoch:74 | Iter:   20 | Time: 01:16:57 | Train_A Loss: 0.1717 | Train_B Loss: 0.1779\n",
      "Epoch:74 | Iter:   40 | Time: 01:17:10 | Train_A Loss: 0.1855 | Train_B Loss: 0.1824\n",
      "Epoch:74 | Iter:   60 | Time: 01:17:23 | Train_A Loss: 0.1773 | Train_B Loss: 0.1664\n",
      "\n",
      " EPOCH 74/100 \t Avg. Train_A loss this Epoch 0.17084147036075592 \t Avg. Train_B loss this Epoch 0.1726532280445099 \t Test loss A 0.10424675047397614 \t Test loss B 0.10029798746109009\n",
      "Epoch:75 | Iter:    0 | Time: 01:17:47 | Train_A Loss: 0.1723 | Train_B Loss: 0.1704\n",
      "Epoch:75 | Iter:   20 | Time: 01:18:01 | Train_A Loss: 0.1794 | Train_B Loss: 0.1686\n",
      "Epoch:75 | Iter:   40 | Time: 01:18:14 | Train_A Loss: 0.1629 | Train_B Loss: 0.1696\n",
      "Epoch:75 | Iter:   60 | Time: 01:18:27 | Train_A Loss: 0.1655 | Train_B Loss: 0.1735\n",
      "\n",
      " EPOCH 75/100 \t Avg. Train_A loss this Epoch 0.171122208237648 \t Avg. Train_B loss this Epoch 0.17272542417049408 \t Test loss A 0.09738588333129883 \t Test loss B 0.09589828550815582\n",
      "Epoch:76 | Iter:    0 | Time: 01:18:51 | Train_A Loss: 0.1626 | Train_B Loss: 0.1706\n",
      "Epoch:76 | Iter:   20 | Time: 01:19:04 | Train_A Loss: 0.1841 | Train_B Loss: 0.1850\n",
      "Epoch:76 | Iter:   40 | Time: 01:19:17 | Train_A Loss: 0.1759 | Train_B Loss: 0.1767\n",
      "Epoch:76 | Iter:   60 | Time: 01:19:30 | Train_A Loss: 0.1756 | Train_B Loss: 0.1632\n",
      "\n",
      " EPOCH 76/100 \t Avg. Train_A loss this Epoch 0.1723446547985077 \t Avg. Train_B loss this Epoch 0.172635018825531 \t Test loss A 0.09216734766960144 \t Test loss B 0.09229360520839691\n",
      "Epoch:77 | Iter:    0 | Time: 01:19:55 | Train_A Loss: 0.1622 | Train_B Loss: 0.1641\n",
      "Epoch:77 | Iter:   20 | Time: 01:20:08 | Train_A Loss: 0.1771 | Train_B Loss: 0.1658\n",
      "Epoch:77 | Iter:   40 | Time: 01:20:21 | Train_A Loss: 0.1702 | Train_B Loss: 0.1629\n",
      "Epoch:77 | Iter:   60 | Time: 01:20:34 | Train_A Loss: 0.1815 | Train_B Loss: 0.1774\n",
      "\n",
      " EPOCH 77/100 \t Avg. Train_A loss this Epoch 0.17217883467674255 \t Avg. Train_B loss this Epoch 0.17268416285514832 \t Test loss A 0.09754735976457596 \t Test loss B 0.09925095736980438\n",
      "Epoch:78 | Iter:    0 | Time: 01:20:59 | Train_A Loss: 0.1673 | Train_B Loss: 0.1694\n",
      "Epoch:78 | Iter:   20 | Time: 01:21:12 | Train_A Loss: 0.1732 | Train_B Loss: 0.1542\n",
      "Epoch:78 | Iter:   40 | Time: 01:21:25 | Train_A Loss: 0.1569 | Train_B Loss: 0.1760\n",
      "Epoch:78 | Iter:   60 | Time: 01:21:38 | Train_A Loss: 0.1749 | Train_B Loss: 0.1580\n",
      "\n",
      " EPOCH 78/100 \t Avg. Train_A loss this Epoch 0.17186479270458221 \t Avg. Train_B loss this Epoch 0.16913050413131714 \t Test loss A 0.10337959229946136 \t Test loss B 0.1021752655506134\n",
      "Epoch:79 | Iter:    0 | Time: 01:22:03 | Train_A Loss: 0.1738 | Train_B Loss: 0.1627\n",
      "Epoch:79 | Iter:   20 | Time: 01:22:16 | Train_A Loss: 0.1812 | Train_B Loss: 0.1713\n",
      "Epoch:79 | Iter:   40 | Time: 01:22:29 | Train_A Loss: 0.1720 | Train_B Loss: 0.1772\n",
      "Epoch:79 | Iter:   60 | Time: 01:22:42 | Train_A Loss: 0.1767 | Train_B Loss: 0.1865\n",
      "\n",
      " EPOCH 79/100 \t Avg. Train_A loss this Epoch 0.17207206785678864 \t Avg. Train_B loss this Epoch 0.16982445120811462 \t Test loss A 0.1025085374712944 \t Test loss B 0.1046709343791008\n",
      "Epoch:80 | Iter:    0 | Time: 01:23:07 | Train_A Loss: 0.1716 | Train_B Loss: 0.1644\n",
      "Epoch:80 | Iter:   20 | Time: 01:23:20 | Train_A Loss: 0.1766 | Train_B Loss: 0.1752\n",
      "Epoch:80 | Iter:   40 | Time: 01:23:33 | Train_A Loss: 0.1612 | Train_B Loss: 0.1784\n",
      "Epoch:80 | Iter:   60 | Time: 01:23:46 | Train_A Loss: 0.1497 | Train_B Loss: 0.1672\n",
      "\n",
      " EPOCH 80/100 \t Avg. Train_A loss this Epoch 0.16973482072353363 \t Avg. Train_B loss this Epoch 0.17069397866725922 \t Test loss A 0.09158012270927429 \t Test loss B 0.09444347023963928\n",
      "Epoch:81 | Iter:    0 | Time: 01:24:11 | Train_A Loss: 0.1518 | Train_B Loss: 0.1702\n",
      "Epoch:81 | Iter:   20 | Time: 01:24:24 | Train_A Loss: 0.1719 | Train_B Loss: 0.1593\n",
      "Epoch:81 | Iter:   40 | Time: 01:24:38 | Train_A Loss: 0.1527 | Train_B Loss: 0.1781\n",
      "Epoch:81 | Iter:   60 | Time: 01:24:50 | Train_A Loss: 0.1926 | Train_B Loss: 0.1772\n",
      "\n",
      " EPOCH 81/100 \t Avg. Train_A loss this Epoch 0.17028293013572693 \t Avg. Train_B loss this Epoch 0.17181721329689026 \t Test loss A 0.09809421002864838 \t Test loss B 0.10052222013473511\n",
      "Epoch:82 | Iter:    0 | Time: 01:25:15 | Train_A Loss: 0.1825 | Train_B Loss: 0.1580\n",
      "Epoch:82 | Iter:   20 | Time: 01:25:28 | Train_A Loss: 0.1710 | Train_B Loss: 0.1896\n",
      "Epoch:82 | Iter:   40 | Time: 01:25:42 | Train_A Loss: 0.1725 | Train_B Loss: 0.1750\n",
      "Epoch:82 | Iter:   60 | Time: 01:25:55 | Train_A Loss: 0.1580 | Train_B Loss: 0.1711\n"
     ]
    }
   ],
   "source": [
    "train_epoch(model,device, train_dl,loss_fn,optim, test_dl, pubfig, epochs=EPOCHS, video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h.saveWeights(model, \"./1024LatentGPU73Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://pythonprogramming.altervista.org/png-to-git-to-tell-a-story-with-python-and-pil/?doing_wp_cron=1682123526.3413488864898681640625\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    " \n",
    "# Create the frames\n",
    "frames = []\n",
    "imgs = sorted(glob.glob('./Outputs/Video/64Adversarial/a/*.png'), key=os.path.getmtime)\n",
    "for i in imgs:\n",
    "    new_frame = Image.open(i)\n",
    "    frames.append(new_frame)\n",
    " \n",
    "# Save into a GIF file that loops forever\n",
    "frames[0].save('double.gif', format='GIF',\n",
    "               append_images=frames[1:],\n",
    "               save_all=True,\n",
    "               duration=5, loop=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
